{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, r2_score\n",
    "from scipy.stats import spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Query LOINC Code  \\\n",
      "0      bilirubin in plasma     1971-1   \n",
      "1         glucose in blood     1920-8   \n",
      "2         glucose in blood     1751-7   \n",
      "3  white blood cells count     2069-3   \n",
      "4      bilirubin in plasma    54439-5   \n",
      "\n",
      "                                      Name                     Component  \\\n",
      "0          bilirubin indirect serum plasma  bilirubin non glucuronidated   \n",
      "1  aspartate aminotransferase serum plasma    aspartate aminotransferase   \n",
      "2                     albumin serum plasma                       albumin   \n",
      "3                           chloride blood                      chloride   \n",
      "4         calcium bilirubinate total stone    calcium bilirubinate total   \n",
      "\n",
      "         System                 Property                Measurement  \\\n",
      "0  serum plasma       mass concentration                mass volume   \n",
      "1  serum plasma       cell concentration  enzymatic activity volume   \n",
      "2  serum plasma       mass concentration                mass volume   \n",
      "3         blood  substance concentration                mole volume   \n",
      "4      calculus                      mfr                        NaN   \n",
      "\n",
      "   Normalized_Score  Query_encoded  Name_encoded  Component_encoded  \\\n",
      "0          0.599738              0            14                 14   \n",
      "1          0.221357              1            11                 10   \n",
      "2          0.155101              1             5                  3   \n",
      "3          0.384412              2            32                 27   \n",
      "4          0.350481              0            23                 21   \n",
      "\n",
      "   System_encoded  Property_encoded  Measurement_encoded  Score_label  \n",
      "0              10                 3                    4            5  \n",
      "1              10                 1                    0            2  \n",
      "2              10                 3                    4            1  \n",
      "3               0                11                    5            3  \n",
      "4               3                 6                   -1            3  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_with_scores.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for col in [\"Query\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\"]:\n",
    "    df[f\"{col}_encoded\"] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score_label'] = (df['Normalized_Score'] * 10).astype(int) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  160\n",
      "Test data size:  41\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "198              0            29                 24              10   \n",
      "38               2            37                 30              10   \n",
      "24               1            24                 22              10   \n",
      "122              2             1                  0               1   \n",
      "196              0            55                 46               0   \n",
      "\n",
      "     Property_encoded  Measurement_encoded  \n",
      "198                11                    5  \n",
      "38                  0                    9  \n",
      "24                 11                    5  \n",
      "122                14                    8  \n",
      "196                 7                   -1  \n",
      "     Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "95               2             2                  1               0   \n",
      "15               2             6                  5              10   \n",
      "30               1            33                 27              10   \n",
      "158              1            19                 16               2   \n",
      "128              0            15                 12              10   \n",
      "\n",
      "     Property_encoded  Measurement_encoded  \n",
      "95                 14                    8  \n",
      "15                  1                    0  \n",
      "30                 11                    5  \n",
      "158                14                    8  \n",
      "128                 3                    4  \n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"Query_encoded\", \"Name_encoded\", \"Component_encoded\",\n",
    "    \"System_encoded\", \"Property_encoded\", \"Measurement_encoded\"\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Score_label\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Score_label\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"rank_xendcg\",\n",
    "    \"metric\": \"ndcg\",  \n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 127,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 20,\n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.05,\n",
    "    \"lambda_l2\": 0.05,\n",
    "    \"colsample_bytree\": 0.9\n",
    "}\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to 'ranked_results.csv'\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted_Score\"] = y_pred\n",
    "df_test[\"Actual_Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "\n",
    "df_test = df_test[[\"Query\", \"Name\", \"Predicted_Score\", \"Actual_Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted_Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(\"ranked_results.csv\", index=False)\n",
    "\n",
    "print(\"Ranked results saved to 'ranked_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4634\n",
      "F1 Score: 0.6333\n",
      "Mean Squared Error (MSE): 0.0125\n",
      "R-squared (R²): 0.7362\n",
      "Spearman's Rank Correlation: 0.7907\n"
     ]
    }
   ],
   "source": [
    "y_true = df_test.loc[X_test.index, \"Actual_Score\"]  \n",
    "\n",
    "margin = 0.05\n",
    "\n",
    "y_pred_adjusted = np.abs(y_pred - y_true) <= margin  \n",
    "y_true_adjusted = np.abs(y_true - y_true) <= margin  \n",
    "\n",
    "accuracy = accuracy_score(y_true_adjusted, y_pred_adjusted)\n",
    "f1 = f1_score(y_true_adjusted, y_pred_adjusted)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
