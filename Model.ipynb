{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation, reset_parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import ndcg_score\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "1. **Reading the CSV File:**\n",
    "    - The code reads a CSV file into a Pandas DataFrame.\n",
    "\n",
    "2. **Shuffling the Data:**\n",
    "    - It shuffles the DataFrame's rows randomly using `sample()` and resets the index.\n",
    "\n",
    "3. **Encoding Categorical Columns:**\n",
    "    - A loop encodes several categorical columns (e.g., \"Query\", \"Name\", etc.) into integer values and creates new columns with the encoded values.\n",
    "\n",
    "4. **Creating a 'Score_label' Column:**\n",
    "    - It generates a new `Score_label` column by scaling and converting the `Normalized_Score` to integers.\n",
    "\n",
    "5. **Displaying the Data:**\n",
    "    - Finally, the first few rows of the updated DataFrame are printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Query LOINC Code  \\\n",
      "0      bilirubin in plasma     1971-1   \n",
      "1         glucose in blood     1920-8   \n",
      "2         glucose in blood     1751-7   \n",
      "3  white blood cells count     2069-3   \n",
      "4      bilirubin in plasma    54439-5   \n",
      "\n",
      "                                      Name                     Component  \\\n",
      "0          bilirubin indirect serum plasma  bilirubin non glucuronidated   \n",
      "1  aspartate aminotransferase serum plasma    aspartate aminotransferase   \n",
      "2                     albumin serum plasma                       albumin   \n",
      "3                           chloride blood                      chloride   \n",
      "4         calcium bilirubinate total stone    calcium bilirubinate total   \n",
      "\n",
      "         System                 Property                Measurement  \\\n",
      "0  serum plasma       mass concentration                mass volume   \n",
      "1  serum plasma       cell concentration  enzymatic activity volume   \n",
      "2  serum plasma       mass concentration                mass volume   \n",
      "3         blood  substance concentration                mole volume   \n",
      "4      calculus                      mfr                        NaN   \n",
      "\n",
      "   Normalized_Score  Query_encoded  Name_encoded  Component_encoded  \\\n",
      "0          0.679552              0            14                 14   \n",
      "1          0.157564              1            11                 10   \n",
      "2          0.088003              1             5                  3   \n",
      "3          0.253916              2            32                 27   \n",
      "4          0.417859              0            23                 21   \n",
      "\n",
      "   System_encoded  Property_encoded  Measurement_encoded  Score_label  \n",
      "0              10                 3                    4            4  \n",
      "1              10                 1                    0            1  \n",
      "2              10                 3                    4            0  \n",
      "3               0                11                    5            2  \n",
      "4               3                 6                   -1            4  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_scores_enhanced.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for col in [\"Query\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\"]:\n",
    "    df[f\"{col}_encoded\"] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score_label'] = (df['Normalized_Score'] * 10).astype(int).clip(0, 4) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "1. **Splitting the Data:**\n",
    "    - The `train_test_split()` function is used to split the DataFrame `df` into training and testing sets.\n",
    "    - 80% of the data is used for training (`train_data`), and 20% is reserved for testing (`test_data`).\n",
    "\n",
    "2. **Displaying Data Sizes:**\n",
    "    - The sizes of the training and testing sets are printed using `shape[0]`, which gives the number of rows in each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  160\n",
      "Test data size:  41\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for trainning\n",
    "\n",
    "1. **Defining Features:**\n",
    "    - A list of feature columns (`features`) is created, which includes the encoded versions of categorical columns (e.g., \"Query_encoded\", \"Name_encoded\", etc.).\n",
    "\n",
    "2. **Preparing Training Data:**\n",
    "    - `X_train` contains the feature values from `train_data`.\n",
    "    - `y_train` contains the target values (`Score_label`) from `train_data`.\n",
    "    - `q_train` calculates the size of each group based on the \"Query_encoded\" column using `groupby()`.\n",
    "\n",
    "3. **Preparing Testing Data:**\n",
    "    - `X_test` contains the feature values from `test_data`.\n",
    "    - `y_test` contains the target values (`Score_label`) from `test_data`.\n",
    "    - `q_test` calculates the size of each group based on the \"Query_encoded\" column for the test data.\n",
    "\n",
    "4. **Creating LightGBM Datasets:**\n",
    "    - `train_data` and `test_data` are converted into LightGBM datasets (`lgb.Dataset`), which are required for training a LightGBM model. These datasets include the feature data (`X_train`, `X_test`), target labels (`y_train`, `y_test`), and group sizes (`q_train`, `q_test`).\n",
    "\n",
    "5. **Displaying Training and Testing Data:**\n",
    "    - The first few rows of `X_train` and `X_test` are printed to verify the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "198              0            29                 24              10   \n",
      "38               2            37                 30              10   \n",
      "24               1            24                 22              10   \n",
      "122              2             1                  0               1   \n",
      "196              0            55                 46               0   \n",
      "\n",
      "     Property_encoded  Measurement_encoded  \n",
      "198                11                    5  \n",
      "38                  0                    9  \n",
      "24                 11                    5  \n",
      "122                14                    8  \n",
      "196                 7                   -1  \n",
      "     Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "95               2             2                  1               0   \n",
      "15               2             6                  5              10   \n",
      "30               1            33                 27              10   \n",
      "158              1            19                 16               2   \n",
      "128              0            15                 12              10   \n",
      "\n",
      "     Property_encoded  Measurement_encoded  \n",
      "95                 14                    8  \n",
      "15                  1                    0  \n",
      "30                 11                    5  \n",
      "158                14                    8  \n",
      "128                 3                    4  \n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"Query_encoded\", \"Name_encoded\", \"Component_encoded\",\n",
    "    \"System_encoded\", \"Property_encoded\", \"Measurement_encoded\"\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Score_label\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Score_label\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "1. **Setting Hyperparameters:**\n",
    "    - A dictionary `params` is defined to specify the hyperparameters for training a LightGBM model. These include:\n",
    "        - `objective`: The type of learning task (\"rank_xendcg\" for ranking tasks).\n",
    "        - `metric`: Evaluation metric used (\"ndcg\" for normalized discounted cumulative gain).\n",
    "        - `boosting_type`: Boosting method (\"gbdt\" for Gradient Boosting Decision Trees).\n",
    "        - `num_leaves`: Number of leaves in the tree.\n",
    "        - `learning_rate`: Step size for each iteration.\n",
    "        - `max_depth`: Maximum depth of the tree.\n",
    "        - `verbosity`: Controls the amount of output during training.\n",
    "        - `lambda_l1` and `lambda_l2`: L1 and L2 regularization terms.\n",
    "        - `colsample_bytree`: Fraction of features to be used for each tree.\n",
    "\n",
    "2. **Training the LightGBM Model:**\n",
    "    - The `lgb.train()` function is used to train the LightGBM model with the specified `params`, using `train_data` as the training dataset.\n",
    "    - `valid_sets=[test_data]` specifies the test dataset to be used for validation during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\tvalid_0's ndcg@1: 0.666667\tvalid_0's ndcg@2: 0.659599\tvalid_0's ndcg@3: 0.660931\tvalid_0's ndcg@4: 0.739968\tvalid_0's ndcg@5: 0.741579\n",
      "[200]\tvalid_0's ndcg@1: 0.666667\tvalid_0's ndcg@2: 0.659599\tvalid_0's ndcg@3: 0.673769\tvalid_0's ndcg@4: 0.752198\tvalid_0's ndcg@5: 0.785095\n",
      "[300]\tvalid_0's ndcg@1: 0.355556\tvalid_0's ndcg@2: 0.570895\tvalid_0's ndcg@3: 0.598613\tvalid_0's ndcg@4: 0.681053\tvalid_0's ndcg@5: 0.715068\n",
      "[400]\tvalid_0's ndcg@1: 0.355556\tvalid_0's ndcg@2: 0.570895\tvalid_0's ndcg@3: 0.598613\tvalid_0's ndcg@4: 0.681053\tvalid_0's ndcg@5: 0.715068\n",
      "[500]\tvalid_0's ndcg@1: 0.355556\tvalid_0's ndcg@2: 0.570895\tvalid_0's ndcg@3: 0.598613\tvalid_0's ndcg@4: 0.681053\tvalid_0's ndcg@5: 0.715068\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's ndcg@1: 0.666667\tvalid_0's ndcg@2: 0.659599\tvalid_0's ndcg@3: 0.660931\tvalid_0's ndcg@4: 0.729434\tvalid_0's ndcg@5: 0.731192\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"rank_xendcg\",  \n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 20,  \n",
    "    \"learning_rate\": 0.05,  \n",
    "    \"max_depth\": 6,  \n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.001,  \n",
    "    \"lambda_l2\": 0.001,\n",
    "    \"colsample_bytree\": 0.9,  \n",
    "    \"subsample\": 0.8,\n",
    "    \"min_child_samples\": 20,  \n",
    "    \"min_child_weight\": 0.01,\n",
    "    \"max_position\": 10,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"label_gain\": [0, 1, 3, 7, 15],  \n",
    "    \"n_estimators\": 10000,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[test_data],\n",
    "    num_boost_round=5000,\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=500), \n",
    "        log_evaluation(100),\n",
    "        reset_parameter(learning_rate=lambda iter: 0.05 * (0.99 ** iter))\n",
    "]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "1. **Making Predictions:**\n",
    "    - `y_pred = model.predict(X_test)` uses the trained LightGBM model to make predictions on the test data (`X_test`).\n",
    "\n",
    "2. **Scaling the Predictions:**\n",
    "    - A `MinMaxScaler` is initialized to scale the predictions (`y_pred`) to a range between 0 and 1.\n",
    "    - The predictions are reshaped, scaled, and then flattened back to the original shape.\n",
    "\n",
    "3. **Creating a DataFrame for the Test Data:**\n",
    "    - `df_test` is created by copying `X_test`.\n",
    "    - New columns are added to `df_test`:\n",
    "        - `Predicted_Score`: The scaled predictions.\n",
    "        - `Actual_Score`: The actual values from the `Normalized_Score` column in the original DataFrame (`df`).\n",
    "        - `Name` and `Query`: The corresponding values from the original DataFrame (`df`).\n",
    "\n",
    "4. **Sorting the Results:**\n",
    "    - The DataFrame is sorted by `Query` and `Predicted_Score` in ascending order by `Query` and descending order by `Predicted_Score`.\n",
    "\n",
    "5. **Saving the Results:**\n",
    "    - The sorted DataFrame `df_test` is saved to a CSV file called \"ranked_results.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to {output_filename}\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"results.csv\"\n",
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted Score\"] = y_pred\n",
    "df_test[\"Actual Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "df_test[\"LOINC Code\"] = df.loc[X_test.index, \"LOINC Code\"]\n",
    "\n",
    "df_test = df_test[[\"LOINC Code\", \"Query\", \"Name\", \"Predicted Score\", \"Actual Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(output_filename, mode='a', index=False, header=not os.path.exists(output_filename))\n",
    "\n",
    "print(\"Ranked results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "1. **Extracting True Scores:**\n",
    "    - Extract the true scores (`Actual_Score`) from the `df_test` DataFrame corresponding to the indices of `X_test`.\n",
    "\n",
    "2. **Adjusting Predictions and True Scores:**\n",
    "    - A margin of 0.05 is defined to determine how close the predicted values should be to the actual values.\n",
    "    - `y_pred_adjusted` checks if the absolute difference between predicted and actual scores is within the margin, essentially marking whether the prediction is considered \"correct.\"\n",
    "    - `y_true_adjusted` checks if the true scores are within the margin of themselves (which will always be `True`, so this step doesn’t affect the results).\n",
    "\n",
    "3. **Calculating Accuracy and F1 Score:**\n",
    "    - `accuracy_score` calculates the proportion of predictions that are correct based on the margin.\n",
    "    - `f1_score` calculates the F1 score, which is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "4. **Calculating Regression Metrics:**\n",
    "    - `mean_squared_error (MSE)` calculates the average squared difference between predicted and true values, indicating the overall error of the predictions.\n",
    "    - `r2_score (R²)` measures the proportion of variance in the true values that is explained by the model, with values closer to 1 indicating better fit.\n",
    "    - `spearmanr` calculates Spearman’s rank correlation coefficient, measuring the monotonic relationship between predicted and true values. A value close to 1 indicates a strong positive correlation.\n",
    "\n",
    "5. **Calculating NDCG (Normalized Discounted Cumulative Gain):**\n",
    "    - The true and predicted scores are grouped by the \"Query\" column to calculate the ranking scores for each query.\n",
    "    - The `ndcg_score` is calculated for each query by comparing the true and predicted ranked lists. It measures how well the model's ranking matches the true ranking.\n",
    "    - The average NDCG score across all queries is then computed using `np.mean()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.1550\n",
      "R-squared (R²): -2.3101\n",
      "Spearman's Rank Correlation: 0.7134\n",
      "NDCG Mean Score: 0.8219\n",
      "- NDCG for 'bilirubin in plasma': 0.7764\n",
      "- NDCG for 'glucose in blood': 0.7507\n",
      "- NDCG for 'white blood cells count': 0.9388\n"
     ]
    }
   ],
   "source": [
    "y_true = df_test.loc[X_test.index, \"Actual Score\"]  \n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "\n",
    "y_true_grouped = df_test.groupby(\"Query\")[\"Actual Score\"].apply(list).tolist()\n",
    "y_pred_grouped = df_test.groupby(\"Query\")[\"Predicted Score\"].apply(list).tolist()\n",
    "\n",
    "ndcg_scores = [ndcg_score([true], [pred]) for true, pred in zip(y_true_grouped, y_pred_grouped)]\n",
    "ndcg_mean = np.mean(ndcg_scores) \n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}\")\n",
    "print(f\"NDCG Mean Score: {ndcg_mean:.4f}\")\n",
    "\n",
    "queries = df_test.groupby(\"Query\").first().index.tolist()\n",
    "\n",
    "for query, true, pred in zip(queries, y_true_grouped, y_pred_grouped):\n",
    "    ndcg = ndcg_score([true], [pred])\n",
    "    print(f\"- NDCG for '{query}': {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
