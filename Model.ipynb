{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation, reset_parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import ndcg_score\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "1. **Reading the CSV File:**\n",
    "    - The code reads a CSV file into a Pandas DataFrame.\n",
    "\n",
    "2. **Shuffling the Data:**\n",
    "    - It shuffles the DataFrame's rows randomly using `sample()` and resets the index.\n",
    "\n",
    "3. **Encoding Categorical Columns:**\n",
    "    - A loop encodes several categorical columns (e.g., \"Query\", \"Name\", etc.) into integer values and creates new columns with the encoded values.\n",
    "\n",
    "4. **Creating a 'Score_label' Column:**\n",
    "    - It generates a new `Score_label` column by scaling and converting the `Normalized_Score` to integers.\n",
    "\n",
    "5. **Displaying the Data:**\n",
    "    - Finally, the first few rows of the updated DataFrame are printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Query LOINC Code  \\\n",
      "0  bilirubin in plasma    21645-7   \n",
      "1     calcium in serum    33389-8   \n",
      "2  bilirubin in plasma    14818-9   \n",
      "3     glucose in blood    12913-0   \n",
      "4     calcium in serum     5449-4   \n",
      "\n",
      "                                                Name  \\\n",
      "0  cacna1s gene mutation tested blood tissue mole...   \n",
      "1               monocyte leukocyte pericardial fluid   \n",
      "2  lymphocyte leukocyte pericardial fluid manual ...   \n",
      "3  beta n acetylhexosaminidase enzymatic activity...   \n",
      "4                deprecated cd21 cell 100 cell blood   \n",
      "\n",
      "                      Component          System              Property  \\\n",
      "0  cacna1s gene mutation tested    blood tissue                  prid   \n",
      "1            monocyte leukocyte  pericard fluid                   nfr   \n",
      "2          lymphocyte leukocyte  pericard fluid                   nfr   \n",
      "3   beta n acetylhexosaminidase             wbc    cell concentration   \n",
      "4                          cd21             wbc  amount concentration   \n",
      "\n",
      "                 Measurement  Normalized_Score  Query_encoded  Name_encoded  \\\n",
      "0                        NaN          0.052633              0           247   \n",
      "1                        NaN          0.087175              1          2312   \n",
      "2                        NaN          0.080985              0          2194   \n",
      "3  enzymatic activity volume          0.102574              2           125   \n",
      "4                        NaN          0.065104              1           740   \n",
      "\n",
      "   Component_encoded  System_encoded  Property_encoded  Measurement_encoded  \\\n",
      "0                 87              17                30                   -1   \n",
      "1               1092              42                24                   -1   \n",
      "2               1062              42                24                   -1   \n",
      "3                 50              81                 2                    9   \n",
      "4                253              81                 0                   -1   \n",
      "\n",
      "   Score  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      1  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_scores_enhanced_1.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for col in [\"Query\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\"]:\n",
    "    df[f\"{col}_encoded\"] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score'] = (df['Normalized_Score'] * 10).astype(int).clip(0, 4) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "1. **Splitting the Data:**\n",
    "    - The `train_test_split()` function is used to split the DataFrame `df` into training and testing sets.\n",
    "    - 80% of the data is used for training (`train_data`), and 20% is reserved for testing (`test_data`).\n",
    "\n",
    "2. **Displaying Data Sizes:**\n",
    "    - The sizes of the training and testing sets are printed using `shape[0]`, which gives the number of rows in each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  8199\n",
      "Test data size:  2733\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Train data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for trainning\n",
    "\n",
    "1. **Defining Features:**\n",
    "    - A list of feature columns (`features`) is created, which includes the encoded versions of categorical columns (e.g., \"Query_encoded\", \"Name_encoded\", etc.).\n",
    "\n",
    "2. **Preparing Training Data:**\n",
    "    - `X_train` contains the feature values from `train_data`.\n",
    "    - `y_train` contains the target values (`Score_label`) from `train_data`.\n",
    "    - `q_train` calculates the size of each group based on the \"Query_encoded\" column using `groupby()`.\n",
    "\n",
    "3. **Preparing Testing Data:**\n",
    "    - `X_test` contains the feature values from `test_data`.\n",
    "    - `y_test` contains the target values (`Score_label`) from `test_data`.\n",
    "    - `q_test` calculates the size of each group based on the \"Query_encoded\" column for the test data.\n",
    "\n",
    "4. **Creating LightGBM Datasets:**\n",
    "    - `train_data` and `test_data` are converted into LightGBM datasets (`lgb.Dataset`), which are required for training a LightGBM model. These datasets include the feature data (`X_train`, `X_test`), target labels (`y_train`, `y_test`), and group sizes (`q_train`, `q_test`).\n",
    "\n",
    "5. **Displaying Training and Testing Data:**\n",
    "    - The first few rows of `X_train` and `X_test` are printed to verify the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "3325              1          1712                877              60   \n",
      "3229              2          2634               1245              81   \n",
      "6839              1          1308                762              60   \n",
      "211               0          2331               1092              74   \n",
      "6321              1          2131               1016              34   \n",
      "\n",
      "      Property_encoded  Measurement_encoded  \n",
      "3325                43                   25  \n",
      "3229                 1                    6  \n",
      "6839                15                   18  \n",
      "211                 24                   -1  \n",
      "6321                29                   37  \n",
      "      Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "533               1          1322                766              60   \n",
      "3883              1          2602               1213              22   \n",
      "932               3          1365                826              60   \n",
      "6946              2           691               1105              25   \n",
      "4439              2          2446               1123              66   \n",
      "\n",
      "      Property_encoded  Measurement_encoded  \n",
      "533                 15                   18  \n",
      "3883                24                   -1  \n",
      "932                 15                   18  \n",
      "6946                 9                   -1  \n",
      "4439                24                   -1  \n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"Query_encoded\", \"Name_encoded\", \"Component_encoded\",\n",
    "    \"System_encoded\", \"Property_encoded\", \"Measurement_encoded\"\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Score\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Score\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train, free_raw_data=False)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data, free_raw_data=False)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "1. **Setting Hyperparameters:**\n",
    "    - The model uses `rank_xendcg` as the objective and `ndcg` as the evaluation metric\n",
    "    - A dictionary `params` is defined to specify the hyperparameters for training a LightGBM model. These include:\n",
    "        - `objective`: The type of learning task (\"rank_xendcg\" for ranking tasks).\n",
    "        - `metric`: Evaluation metric used (\"ndcg\" for normalized discounted cumulative gain).\n",
    "        - `boosting_type`: Boosting method (\"gbdt\" for Gradient Boosting Decision Trees).\n",
    "        - `num_leaves`: Number of leaves in the tree.\n",
    "        - `learning_rate`: Step size for each iteration.\n",
    "        - `max_depth`: Maximum depth of the tree.\n",
    "        - `verbosity`: Controls the amount of output during training.\n",
    "        - `lambda_l1` and `lambda_l2`: L1 and L2 regularization terms.\n",
    "        - `colsample_bytree`: Fraction of features to be used for each tree.\n",
    "        - `label_gain`: Prioritizes higher-ranking samples.\n",
    "        - `n_estimators`: sets a high number of rounds for better performance\n",
    "\n",
    "2. **Adaptive Training with AdaRank:**\n",
    "    \n",
    "    Since LightGBM doesn’t directly support *AdaRank*, this code simulates its boosting and reweighting mechanism using scikit-learn’s AdaBoost (which supports custom regressors/rankers) and *LightGBM's* as the base model.  \n",
    "    - The model trains over multiple iterations (`n_iterations`), updating weights based on prediction errors.  \n",
    "    - Misclassified samples get higher weights, forcing the model to focus on harder cases.  \n",
    "    - **Early stopping** prevents overfitting, while **log evaluation** tracks performance every 100 rounds.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaRank Iteration 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 6\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 7\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 0.957968\tvalid_0's ndcg@5: 0.963483\n",
      "AdaRank Iteration 8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 0.957968\tvalid_0's ndcg@5: 0.963483\n",
      "AdaRank Iteration 9\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    \"objective\": \"rank_xendcg\",  \n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,  \n",
    "    \"learning_rate\": 0.05,  \n",
    "    \"max_depth\": 7,  \n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.001,  \n",
    "    \"lambda_l2\": 0.001,\n",
    "    \"colsample_bytree\": 0.9,  \n",
    "    \"subsample\": 0.8,\n",
    "    \"min_child_samples\": 20,  \n",
    "    \"min_child_weight\": 0.01,\n",
    "    \"max_position\": 10,\n",
    "    \"feature_fraction\": 0.95,\n",
    "    \"label_gain\": [0, 1, 3, 7, 15],\n",
    "    \"n_estimators\": 10000,\n",
    "}\n",
    "\n",
    "n_iterations = 10\n",
    "weights = np.ones(len(X_train))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"AdaRank Iteration {i + 1}\")\n",
    "\n",
    "    model = lgb.train(\n",
    "        base_params,\n",
    "        train_data,\n",
    "        valid_sets=[test_data],\n",
    "        num_boost_round=500,\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    errors = np.abs(y_train - y_pred)\n",
    "    weights *= 1 + (errors / (errors.std() + 1e-6)) * 0.01\n",
    "    weights = np.log1p(weights)\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, group=q_train, weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "1. **Making Predictions:**\n",
    "    - `y_pred = model.predict(X_test)` uses the trained LightGBM model to make predictions on the test data (`X_test`).\n",
    "\n",
    "2. **Scaling the Predictions:**\n",
    "    - A `MinMaxScaler` is initialized to scale the predictions (`y_pred`) to a range between 0 and 1.\n",
    "    - The predictions are reshaped, scaled, and then flattened back to the original shape.\n",
    "\n",
    "3. **Creating a DataFrame for the Test Data:**\n",
    "    - `df_test` is created by copying `X_test`.\n",
    "    - New columns are added to `df_test`:\n",
    "        - `Predicted_Score`: The scaled predictions.\n",
    "        - `Actual_Score`: The actual values from the `Normalized_Score` column in the original DataFrame (`df`).\n",
    "        - `Name` and `Query`: The corresponding values from the original DataFrame (`df`).\n",
    "\n",
    "4. **Sorting the Results:**\n",
    "    - The DataFrame is sorted by `Query` and `Predicted_Score` in ascending order by `Query` and descending order by `Predicted_Score`.\n",
    "\n",
    "5. **Saving the Results:**\n",
    "    - The sorted DataFrame `df_test` is saved to a CSV file called \"results.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to {output_filename}\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"results.csv\"\n",
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted Score\"] = y_pred\n",
    "df_test[\"Actual Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "df_test[\"LOINC Code\"] = df.loc[X_test.index, \"LOINC Code\"]\n",
    "\n",
    "df_test = df_test[[\"LOINC Code\", \"Query\", \"Name\", \"Predicted Score\", \"Actual Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(output_filename, mode='a', index=False, header=not os.path.exists(output_filename))\n",
    "\n",
    "print(\"Ranked results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "1. **Extracting True Scores:**\n",
    "    - Extract the true scores (`Actual_Score`) from the `df_test` DataFrame corresponding to the indices of `X_test`.\n",
    "\n",
    "2. **Adjusting Predictions and True Scores:**\n",
    "    - A margin of 0.05 is defined to determine how close the predicted values should be to the actual values.\n",
    "    - `y_pred_adjusted` checks if the absolute difference between predicted and actual scores is within the margin, essentially marking whether the prediction is considered \"correct.\"\n",
    "    - `y_true_adjusted` checks if the true scores are within the margin of themselves (which will always be `True`, so this step doesn’t affect the results).\n",
    "\n",
    "3. **Calculating Accuracy and F1 Score:**\n",
    "    - `accuracy_score` calculates the proportion of predictions that are correct based on the margin.\n",
    "    - `f1_score` calculates the F1 score, which is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "4. **Calculating Regression Metrics:**\n",
    "    - `mean_squared_error (MSE)` calculates the average squared difference between predicted and true values, indicating the overall error of the predictions.\n",
    "    - `r2_score (R²)` measures the proportion of variance in the true values that is explained by the model, with values closer to 1 indicating better fit.\n",
    "    - `spearmanr` calculates Spearman’s rank correlation coefficient, measuring the monotonic relationship between predicted and true values. A value close to 1 indicates a strong positive correlation.\n",
    "\n",
    "5. **Calculating NDCG (Normalized Discounted Cumulative Gain):**\n",
    "    - The true and predicted scores are grouped by the \"Query\" column to calculate the ranking scores for each query.\n",
    "    - The `ndcg_score` is calculated for each query by comparing the true and predicted ranked lists. It measures how well the model's ranking matches the true ranking.\n",
    "    - The average NDCG score across all queries is then computed using `np.mean()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0461\n",
      "R-squared (R²): -0.8984\n",
      "Spearman's Rank Correlation: 0.6024\n",
      "NDCG Mean Score: 0.9421\n",
      "- NDCG for 'bilirubin in plasma': 0.9287\n",
      "- NDCG for 'calcium in serum': 0.9338\n",
      "- NDCG for 'glucose in blood': 0.9383\n",
      "- NDCG for 'white blood cells count': 0.9678\n"
     ]
    }
   ],
   "source": [
    "y_true = df_test.loc[X_test.index, \"Actual Score\"]  \n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "\n",
    "y_true_grouped = df_test.groupby(\"Query\")[\"Actual Score\"].apply(list).tolist()\n",
    "y_pred_grouped = df_test.groupby(\"Query\")[\"Predicted Score\"].apply(list).tolist()\n",
    "\n",
    "ndcg_scores = [ndcg_score([true], [pred]) for true, pred in zip(y_true_grouped, y_pred_grouped)]\n",
    "ndcg_mean = np.mean(ndcg_scores) \n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}\")\n",
    "print(f\"NDCG Mean Score: {ndcg_mean:.4f}\")\n",
    "\n",
    "queries = df_test.groupby(\"Query\").first().index.tolist()\n",
    "\n",
    "for query, true, pred in zip(queries, y_true_grouped, y_pred_grouped):\n",
    "    ndcg = ndcg_score([true], [pred])\n",
    "    print(f\"- NDCG for '{query}': {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
