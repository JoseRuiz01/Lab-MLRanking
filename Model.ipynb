{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation, reset_parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import ndcg_score\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "1. **Reading the CSV File:**\n",
    "    - The code reads a CSV file into a Pandas DataFrame.\n",
    "\n",
    "2. **Shuffling the Data:**\n",
    "    - It shuffles the DataFrame's rows randomly using `sample()` and resets the index.\n",
    "\n",
    "3. **Encoding Categorical Columns:**\n",
    "    - A loop encodes several categorical columns (e.g., \"Query\", \"Name\", etc.) into integer values and creates new columns with the encoded values.\n",
    "\n",
    "4. **Creating a 'Score_label' Column:**\n",
    "    - It generates a new `Score_label` column by scaling and converting the `Normalized_Score` to integers.\n",
    "\n",
    "5. **Displaying the Data:**\n",
    "    - Finally, the first few rows of the updated DataFrame are printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Query LOINC Code  \\\n",
      "0  white blood cells count    30113-5   \n",
      "1      bilirubin in plasma    30423-8   \n",
      "2  white blood cells count    71604-3   \n",
      "3      bilirubin in plasma    70040-1   \n",
      "4      bilirubin in plasma    77135-2   \n",
      "\n",
      "                                                Name                Component  \\\n",
      "0                    igg lymphocyte volume leukocyte           lymphocyte igg   \n",
      "1                      lymphoma cell leukocyte blood  lymphoma cell leukocyte   \n",
      "2  monocyte leukocyte pure number fraction perica...       monocyte leukocyte   \n",
      "3  granulocyte leukocyte synovial fluid automated...    granulocyte leukocyte   \n",
      "4             glucose mole volume serum plasma blood                  glucose   \n",
      "\n",
      "               System                 Property           Measurement  \\\n",
      "0                 wbc     number concentration                volume   \n",
      "1               blood                      nfr                   NaN   \n",
      "2      pericard fluid                   nfr df  pure number fraction   \n",
      "3          synv fluid                      nfr                   NaN   \n",
      "4  serum plasma blood  substance concentration           mole volume   \n",
      "\n",
      "   Normalized_Score  Query_encoded  Name_encoded  Component_encoded  \\\n",
      "0          0.104536              3           450                190   \n",
      "1          0.060264              0           675                199   \n",
      "2          0.098744              3           762                213   \n",
      "3          0.059471              0           431                148   \n",
      "4          0.177763              0           394                129   \n",
      "\n",
      "   System_encoded  Property_encoded  Measurement_encoded  Score  \n",
      "0              49                22                   22      1  \n",
      "1               5                17                   -1      0  \n",
      "2              27                18                   18      0  \n",
      "3              41                17                   -1      0  \n",
      "4              37                28                   14      1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_scores_enhanced_1.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for col in [\"Query\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\"]:\n",
    "    df[f\"{col}_encoded\"] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score'] = (df['Normalized_Score'] * 10).astype(int).clip(0, 4) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "1. **Splitting the Data:**\n",
    "    - The `train_test_split()` function is used to split the DataFrame `df` into training and testing sets.\n",
    "    - 80% of the data is used for training (`train_data`), and 20% is reserved for testing (`test_data`).\n",
    "\n",
    "2. **Displaying Data Sizes:**\n",
    "    - The sizes of the training and testing sets are printed using `shape[0]`, which gives the number of rows in each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  3102\n",
      "Test data size:  1034\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Train data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for trainning\n",
    "\n",
    "1. **Defining Features:**\n",
    "    - A list of feature columns (`features`) is created, which includes the encoded versions of categorical columns (e.g., \"Query_encoded\", \"Name_encoded\", etc.).\n",
    "\n",
    "2. **Preparing Training Data:**\n",
    "    - `X_train` contains the feature values from `train_data`.\n",
    "    - `y_train` contains the target values (`Score_label`) from `train_data`.\n",
    "    - `q_train` calculates the size of each group based on the \"Query_encoded\" column using `groupby()`.\n",
    "\n",
    "3. **Preparing Testing Data:**\n",
    "    - `X_test` contains the feature values from `test_data`.\n",
    "    - `y_test` contains the target values (`Score_label`) from `test_data`.\n",
    "    - `q_test` calculates the size of each group based on the \"Query_encoded\" column for the test data.\n",
    "\n",
    "4. **Creating LightGBM Datasets:**\n",
    "    - `train_data` and `test_data` are converted into LightGBM datasets (`lgb.Dataset`), which are required for training a LightGBM model. These datasets include the feature data (`X_train`, `X_test`), target labels (`y_train`, `y_test`), and group sizes (`q_train`, `q_test`).\n",
    "\n",
    "5. **Displaying Training and Testing Data:**\n",
    "    - The first few rows of `X_train` and `X_test` are printed to verify the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "1366              1           162                 51              36   \n",
      "929               3           171                 58              36   \n",
      "1174              2           985                267              15   \n",
      "2740              1           858                233              21   \n",
      "4115              3           769                213              44   \n",
      "\n",
      "      Property_encoded  Measurement_encoded  \n",
      "1366                10                   12  \n",
      "929                 10                   12  \n",
      "1174                18                   18  \n",
      "2740                17                   -1  \n",
      "4115                17                   -1  \n",
      "      Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "2925              1           908                242              36   \n",
      "1210              2           158                 45              36   \n",
      "2577              3           509                177              18   \n",
      "1752              1           753                213              22   \n",
      "1095              0             6                186              15   \n",
      "\n",
      "      Property_encoded  Measurement_encoded  \n",
      "2925                -1                   -1  \n",
      "1210                10                   12  \n",
      "2577                17                   -1  \n",
      "1752                17                   -1  \n",
      "1095                17                   -1  \n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"Query_encoded\", \"Name_encoded\", \"Component_encoded\",\n",
    "    \"System_encoded\", \"Property_encoded\", \"Measurement_encoded\"\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Score\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Score\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train, free_raw_data=False)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data, free_raw_data=False)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "1. **Setting Hyperparameters:**\n",
    "    - The model uses `rank_xendcg` as the objective and `ndcg` as the evaluation metric\n",
    "    - A dictionary `params` is defined to specify the hyperparameters for training a LightGBM model. These include:\n",
    "        - `objective`: The type of learning task (\"rank_xendcg\" for ranking tasks).\n",
    "        - `metric`: Evaluation metric used (\"ndcg\" for normalized discounted cumulative gain).\n",
    "        - `boosting_type`: Boosting method (\"gbdt\" for Gradient Boosting Decision Trees).\n",
    "        - `num_leaves`: Number of leaves in the tree.\n",
    "        - `learning_rate`: Step size for each iteration.\n",
    "        - `max_depth`: Maximum depth of the tree.\n",
    "        - `verbosity`: Controls the amount of output during training.\n",
    "        - `lambda_l1` and `lambda_l2`: L1 and L2 regularization terms.\n",
    "        - `colsample_bytree`: Fraction of features to be used for each tree.\n",
    "        - `label_gain`: Prioritizes higher-ranking samples.\n",
    "        - `n_estimators`: sets a high number of rounds for better performance\n",
    "\n",
    "2. **Adaptive Training with AdaRank:**\n",
    "    \n",
    "    Since LightGBM doesn’t directly support *AdaRank*, this code simulates its boosting and reweighting mechanism using scikit-learn’s AdaBoost (which supports custom regressors/rankers) and *LightGBM's* as the base model.  \n",
    "    - The model trains over multiple iterations (`n_iterations`), updating weights based on prediction errors.  \n",
    "    - Misclassified samples get higher weights, forcing the model to focus on harder cases.  \n",
    "    - **Early stopping** prevents overfitting, while **log evaluation** tracks performance every 100 rounds.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaRank Iteration 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 0.977583\tvalid_0's ndcg@5: 0.980524\n",
      "AdaRank Iteration 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 0.75\tvalid_0's ndcg@2: 0.769343\tvalid_0's ndcg@3: 0.706144\tvalid_0's ndcg@4: 0.671486\tvalid_0's ndcg@5: 0.622745\n",
      "AdaRank Iteration 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@1: 0.75\tvalid_0's ndcg@2: 0.846713\tvalid_0's ndcg@3: 0.765361\tvalid_0's ndcg@4: 0.678714\tvalid_0's ndcg@5: 0.688067\n",
      "AdaRank Iteration 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 0.75\tvalid_0's ndcg@2: 0.653287\tvalid_0's ndcg@3: 0.503911\tvalid_0's ndcg@4: 0.506055\tvalid_0's ndcg@5: 0.439658\n",
      "AdaRank Iteration 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 0.957968\tvalid_0's ndcg@5: 0.930682\n",
      "AdaRank Iteration 6\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 0.957968\tvalid_0's ndcg@5: 0.89788\n",
      "AdaRank Iteration 7\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 0.903287\tvalid_0's ndcg@3: 0.92598\tvalid_0's ndcg@4: 0.812329\tvalid_0's ndcg@5: 0.738548\n",
      "AdaRank Iteration 8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 0.903287\tvalid_0's ndcg@3: 0.92598\tvalid_0's ndcg@4: 0.812329\tvalid_0's ndcg@5: 0.738548\n",
      "AdaRank Iteration 9\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 0.806574\tvalid_0's ndcg@3: 0.67598\tvalid_0's ndcg@4: 0.646393\tvalid_0's ndcg@5: 0.659986\n",
      "AdaRank Iteration 10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 0.806574\tvalid_0's ndcg@3: 0.67598\tvalid_0's ndcg@4: 0.646393\tvalid_0's ndcg@5: 0.659986\n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    \"objective\": \"rank_xendcg\",  \n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,  \n",
    "    \"learning_rate\": 0.05,  \n",
    "    \"max_depth\": 7,  \n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.001,  \n",
    "    \"lambda_l2\": 0.001,\n",
    "    \"colsample_bytree\": 0.9,  \n",
    "    \"subsample\": 0.8,\n",
    "    \"min_child_samples\": 20,  \n",
    "    \"min_child_weight\": 0.01,\n",
    "    \"max_position\": 10,\n",
    "    \"feature_fraction\": 0.95,\n",
    "    \"label_gain\": [0, 1, 3, 7, 15],\n",
    "    \"n_estimators\": 10000,\n",
    "}\n",
    "\n",
    "n_iterations = 10\n",
    "weights = np.ones(len(X_train))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"AdaRank Iteration {i + 1}\")\n",
    "\n",
    "    model = lgb.train(\n",
    "        base_params,\n",
    "        train_data,\n",
    "        valid_sets=[test_data],\n",
    "        num_boost_round=500,\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    errors = np.abs(y_train - y_pred)\n",
    "    weights *= 1 + (errors / (errors.std() + 1e-6)) * 0.01\n",
    "    weights = np.log1p(weights)\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, group=q_train, weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "1. **Making Predictions:**\n",
    "    - `y_pred = model.predict(X_test)` uses the trained LightGBM model to make predictions on the test data (`X_test`).\n",
    "\n",
    "2. **Scaling the Predictions:**\n",
    "    - A `MinMaxScaler` is initialized to scale the predictions (`y_pred`) to a range between 0 and 1.\n",
    "    - The predictions are reshaped, scaled, and then flattened back to the original shape.\n",
    "\n",
    "3. **Creating a DataFrame for the Test Data:**\n",
    "    - `df_test` is created by copying `X_test`.\n",
    "    - New columns are added to `df_test`:\n",
    "        - `Predicted_Score`: The scaled predictions.\n",
    "        - `Actual_Score`: The actual values from the `Normalized_Score` column in the original DataFrame (`df`).\n",
    "        - `Name` and `Query`: The corresponding values from the original DataFrame (`df`).\n",
    "\n",
    "4. **Sorting the Results:**\n",
    "    - The DataFrame is sorted by `Query` and `Predicted_Score` in ascending order by `Query` and descending order by `Predicted_Score`.\n",
    "\n",
    "5. **Saving the Results:**\n",
    "    - The sorted DataFrame `df_test` is saved to a CSV file called \"results.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to {output_filename}\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"results.csv\"\n",
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted Score\"] = y_pred\n",
    "df_test[\"Actual Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "df_test[\"LOINC Code\"] = df.loc[X_test.index, \"LOINC Code\"]\n",
    "\n",
    "df_test = df_test[[\"LOINC Code\", \"Query\", \"Name\", \"Predicted Score\", \"Actual Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(output_filename, mode='a', index=False, header=not os.path.exists(output_filename))\n",
    "\n",
    "print(\"Ranked results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "1. **Extracting True Scores:**\n",
    "    - Extract the true scores (`Actual_Score`) from the `df_test` DataFrame corresponding to the indices of `X_test`.\n",
    "\n",
    "2. **Adjusting Predictions and True Scores:**\n",
    "    - A margin of 0.05 is defined to determine how close the predicted values should be to the actual values.\n",
    "    - `y_pred_adjusted` checks if the absolute difference between predicted and actual scores is within the margin, essentially marking whether the prediction is considered \"correct.\"\n",
    "    - `y_true_adjusted` checks if the true scores are within the margin of themselves (which will always be `True`, so this step doesn’t affect the results).\n",
    "\n",
    "3. **Calculating Accuracy and F1 Score:**\n",
    "    - `accuracy_score` calculates the proportion of predictions that are correct based on the margin.\n",
    "    - `f1_score` calculates the F1 score, which is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "4. **Calculating Regression Metrics:**\n",
    "    - `mean_squared_error (MSE)` calculates the average squared difference between predicted and true values, indicating the overall error of the predictions.\n",
    "    - `r2_score (R²)` measures the proportion of variance in the true values that is explained by the model, with values closer to 1 indicating better fit.\n",
    "    - `spearmanr` calculates Spearman’s rank correlation coefficient, measuring the monotonic relationship between predicted and true values. A value close to 1 indicates a strong positive correlation.\n",
    "\n",
    "5. **Calculating NDCG (Normalized Discounted Cumulative Gain):**\n",
    "    - The true and predicted scores are grouped by the \"Query\" column to calculate the ranking scores for each query.\n",
    "    - The `ndcg_score` is calculated for each query by comparing the true and predicted ranked lists. It measures how well the model's ranking matches the true ranking.\n",
    "    - The average NDCG score across all queries is then computed using `np.mean()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0479\n",
      "R-squared (R²): -1.9010\n",
      "Spearman's Rank Correlation: 0.4700\n",
      "NDCG Mean Score: 0.8533\n",
      "- NDCG for 'bilirubin in plasma': 0.8916\n",
      "- NDCG for 'calcium in serum': 0.9431\n",
      "- NDCG for 'glucose in blood': 0.6946\n",
      "- NDCG for 'white blood cells count': 0.8838\n"
     ]
    }
   ],
   "source": [
    "y_true = df_test.loc[X_test.index, \"Actual Score\"]  \n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "\n",
    "y_true_grouped = df_test.groupby(\"Query\")[\"Actual Score\"].apply(list).tolist()\n",
    "y_pred_grouped = df_test.groupby(\"Query\")[\"Predicted Score\"].apply(list).tolist()\n",
    "\n",
    "ndcg_scores = [ndcg_score([true], [pred]) for true, pred in zip(y_true_grouped, y_pred_grouped)]\n",
    "ndcg_mean = np.mean(ndcg_scores) \n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}\")\n",
    "print(f\"NDCG Mean Score: {ndcg_mean:.4f}\")\n",
    "\n",
    "queries = df_test.groupby(\"Query\").first().index.tolist()\n",
    "\n",
    "for query, true, pred in zip(queries, y_true_grouped, y_pred_grouped):\n",
    "    ndcg = ndcg_score([true], [pred])\n",
    "    print(f\"- NDCG for '{query}': {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
