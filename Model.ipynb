{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation, reset_parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import ndcg_score\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "1. **Reading the CSV File:**\n",
    "    - The code reads a CSV file into a Pandas DataFrame.\n",
    "\n",
    "2. **Shuffling the Data:**\n",
    "    - It shuffles the DataFrame's rows randomly using `sample()` and resets the index.\n",
    "\n",
    "3. **Encoding Categorical Columns:**\n",
    "    - A loop encodes several categorical columns (e.g., \"Query\", \"Name\", etc.) into integer values and creates new columns with the encoded values.\n",
    "\n",
    "4. **Creating a 'Score_label' Column:**\n",
    "    - It generates a new `Score_label` column by scaling and converting the `Normalized_Score` to integers.\n",
    "\n",
    "5. **Displaying the Data:**\n",
    "    - Finally, the first few rows of the updated DataFrame are printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Query LOINC Code  \\\n",
      "0      bilirubin in plasma    40026-7   \n",
      "1         glucose in blood     5775-2   \n",
      "2  white blood cells count    27334-2   \n",
      "3  white blood cells count    50082-7   \n",
      "4  white blood cells count    75308-7   \n",
      "\n",
      "                                                Name  \\\n",
      "0  glucose mass volume serum plasma 6 minute post...   \n",
      "1  calcium phosphate crystal presence urine sedim...   \n",
      "2                tryptophan unit volume serum plasma   \n",
      "3  18 hydroxycorticosterone mole volume serum plasma   \n",
      "4         fetal blood volume blood flow cytometry fc   \n",
      "\n",
      "                       Component        System                 Property  \\\n",
      "0  glucose 6m post xxx challenge  serum plasma       mass concentration   \n",
      "1      calcium phosphate crystal     urine sed                    prthr   \n",
      "2                     tryptophan  serum plasma     amount concentration   \n",
      "3       18 hydroxycorticosterone  serum plasma  substance concentration   \n",
      "4                    fetal blood         blood                      vol   \n",
      "\n",
      "   Measurement  Normalized_Score  Query_encoded  Name_encoded  \\\n",
      "0  mass volume          0.180744              0          2322   \n",
      "1     presence          0.054067              2           976   \n",
      "2  unit volume          0.060317              3          4401   \n",
      "3  mole volume          0.054078              3            17   \n",
      "4       volume          0.280624              3          1893   \n",
      "\n",
      "   Component_encoded  System_encoded  Property_encoded  Measurement_encoded  \\\n",
      "0               1392             127                24                   21   \n",
      "1                473             155                45                   35   \n",
      "2               2334             127                 1                   44   \n",
      "3                  9             127                60                   28   \n",
      "4               1012              16                71                   45   \n",
      "\n",
      "   Score  \n",
      "0      1  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      2  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_scores_enhanced_3.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for col in [\"Query\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\"]:\n",
    "    df[f\"{col}_encoded\"] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score'] = (df['Normalized_Score'] * 10).astype(int).clip(0, 4) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "1. **Splitting the Data:**\n",
    "    - The `train_test_split()` function is used to split the DataFrame `df` into training and testing sets.\n",
    "    - 80% of the data is used for training (`train_data`), and 20% is reserved for testing (`test_data`).\n",
    "\n",
    "2. **Displaying Data Sizes:**\n",
    "    - The sizes of the training and testing sets are printed using `shape[0]`, which gives the number of rows in each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  13815\n",
      "Test data size:  4605\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Train data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for trainning\n",
    "\n",
    "1. **Defining Features:**\n",
    "    - A list of feature columns (`features`) is created, which includes the encoded versions of categorical columns (e.g., \"Query_encoded\", \"Name_encoded\", etc.).\n",
    "\n",
    "2. **Preparing Training Data:**\n",
    "    - `X_train` contains the feature values from `train_data`.\n",
    "    - `y_train` contains the target values (`Score_label`) from `train_data`.\n",
    "    - `q_train` calculates the size of each group based on the \"Query_encoded\" column using `groupby()`.\n",
    "\n",
    "3. **Preparing Testing Data:**\n",
    "    - `X_test` contains the feature values from `test_data`.\n",
    "    - `y_test` contains the target values (`Score_label`) from `test_data`.\n",
    "    - `q_test` calculates the size of each group based on the \"Query_encoded\" column for the test data.\n",
    "\n",
    "4. **Creating LightGBM Datasets:**\n",
    "    - `train_data` and `test_data` are converted into LightGBM datasets (`lgb.Dataset`), which are required for training a LightGBM model. These datasets include the feature data (`X_train`, `X_test`), target labels (`y_train`, `y_test`), and group sizes (`q_train`, `q_test`).\n",
    "\n",
    "5. **Displaying Training and Testing Data:**\n",
    "    - The first few rows of `X_train` and `X_test` are printed to verify the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "7039               2           245                166             127   \n",
      "10175              3          4089               2144             127   \n",
      "2122               0          2061               1081              68   \n",
      "2673               3          1978               1071              27   \n",
      "14850              2          1204                761             108   \n",
      "\n",
      "       Property_encoded  Measurement_encoded  \n",
      "7039                 60                   28  \n",
      "10175                24                   21  \n",
      "2122                 24                   21  \n",
      "2673                 -1                   -1  \n",
      "14850                45                   35  \n",
      "       Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "4010               3          1709                908             127   \n",
      "9703               1          4285               1670               7   \n",
      "15104              3          1530               1385             124   \n",
      "14634              1          1910               1021             162   \n",
      "15439              0          2782               1496             160   \n",
      "\n",
      "       Property_encoded  Measurement_encoded  \n",
      "4010                 24                   21  \n",
      "9703                 42                   -1  \n",
      "15104                24                   21  \n",
      "14634                20                   14  \n",
      "15439                 4                    8  \n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"Query_encoded\", \"Name_encoded\", \"Component_encoded\",\n",
    "    \"System_encoded\", \"Property_encoded\", \"Measurement_encoded\"\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Score\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Score\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train, free_raw_data=False)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data, free_raw_data=False)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "1. **Setting Hyperparameters:**\n",
    "    - The model uses `rank_xendcg` as the objective and `ndcg` as the evaluation metric\n",
    "    - A dictionary `params` is defined to specify the hyperparameters for training a LightGBM model. These include:\n",
    "        - `objective`: The type of learning task (\"rank_xendcg\" for ranking tasks).\n",
    "        - `metric`: Evaluation metric used (\"ndcg\" for normalized discounted cumulative gain).\n",
    "        - `boosting_type`: Boosting method (\"gbdt\" for Gradient Boosting Decision Trees).\n",
    "        - `num_leaves`: Number of leaves in the tree.\n",
    "        - `learning_rate`: Step size for each iteration.\n",
    "        - `max_depth`: Maximum depth of the tree.\n",
    "        - `verbosity`: Controls the amount of output during training.\n",
    "        - `lambda_l1` and `lambda_l2`: L1 and L2 regularization terms.\n",
    "        - `colsample_bytree`: Fraction of features to be used for each tree.\n",
    "        - `label_gain`: Prioritizes higher-ranking samples.\n",
    "        - `n_estimators`: sets a high number of rounds for better performance\n",
    "\n",
    "2. **Adaptive Training with AdaRank:**\n",
    "    \n",
    "    Since LightGBM doesn’t directly support *AdaRank*, this code simulates its boosting and reweighting mechanism using scikit-learn’s AdaBoost (which supports custom regressors/rankers) and *LightGBM's* as the base model.  \n",
    "    - The model trains over multiple iterations (`n_iterations`), updating weights based on prediction errors.  \n",
    "    - Misclassified samples get higher weights, forcing the model to focus on harder cases.  \n",
    "    - **Early stopping** prevents overfitting, while **log evaluation** tracks performance every 100 rounds.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaRank Iteration 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 6\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 7\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 9\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    \"objective\": \"rank_xendcg\",  \n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,  \n",
    "    \"learning_rate\": 0.05,  \n",
    "    \"max_depth\": 7,  \n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.001,  \n",
    "    \"lambda_l2\": 0.001,\n",
    "    \"colsample_bytree\": 0.9,  \n",
    "    \"subsample\": 0.8,\n",
    "    \"min_child_samples\": 20,  \n",
    "    \"min_child_weight\": 0.01,\n",
    "    \"max_position\": 10,\n",
    "    \"feature_fraction\": 0.95,\n",
    "    \"label_gain\": [0, 1, 3, 7, 15],\n",
    "    \"n_estimators\": 10000,\n",
    "}\n",
    "\n",
    "n_iterations = 10\n",
    "weights = np.ones(len(X_train))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"AdaRank Iteration {i + 1}\")\n",
    "\n",
    "    model = lgb.train(\n",
    "        base_params,\n",
    "        train_data,\n",
    "        valid_sets=[test_data],\n",
    "        num_boost_round=500,\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    errors = np.abs(y_train - y_pred)\n",
    "    weights *= 1 + (errors / (errors.std() + 1e-6)) * 0.01\n",
    "    weights = np.log1p(weights)\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, group=q_train, weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "1. **Making Predictions:**\n",
    "    - `y_pred = model.predict(X_test)` uses the trained LightGBM model to make predictions on the test data (`X_test`).\n",
    "\n",
    "2. **Scaling the Predictions:**\n",
    "    - A `MinMaxScaler` is initialized to scale the predictions (`y_pred`) to a range between 0 and 1.\n",
    "    - The predictions are reshaped, scaled, and then flattened back to the original shape.\n",
    "\n",
    "3. **Creating a DataFrame for the Test Data:**\n",
    "    - `df_test` is created by copying `X_test`.\n",
    "    - New columns are added to `df_test`:\n",
    "        - `Predicted_Score`: The scaled predictions.\n",
    "        - `Actual_Score`: The actual values from the `Normalized_Score` column in the original DataFrame (`df`).\n",
    "        - `Name` and `Query`: The corresponding values from the original DataFrame (`df`).\n",
    "\n",
    "4. **Sorting the Results:**\n",
    "    - The DataFrame is sorted by `Query` and `Predicted_Score` in ascending order by `Query` and descending order by `Predicted_Score`.\n",
    "\n",
    "5. **Saving the Results:**\n",
    "    - The sorted DataFrame `df_test` is saved to a CSV file called \"results.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to {output_filename}\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"results.csv\"\n",
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted Score\"] = y_pred\n",
    "df_test[\"Actual Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "df_test[\"LOINC Code\"] = df.loc[X_test.index, \"LOINC Code\"]\n",
    "\n",
    "df_test = df_test[[\"LOINC Code\", \"Query\", \"Name\", \"Predicted Score\", \"Actual Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(output_filename, mode='a', index=False, header=not os.path.exists(output_filename))\n",
    "\n",
    "print(\"Ranked results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "1. **Extracting True Scores:**\n",
    "    - Extract the true scores (`Actual_Score`) from the `df_test` DataFrame corresponding to the indices of `X_test`.\n",
    "\n",
    "2. **Adjusting Predictions and True Scores:**\n",
    "    - A margin of 0.05 is defined to determine how close the predicted values should be to the actual values.\n",
    "    - `y_pred_adjusted` checks if the absolute difference between predicted and actual scores is within the margin, essentially marking whether the prediction is considered \"correct.\"\n",
    "    - `y_true_adjusted` checks if the true scores are within the margin of themselves (which will always be `True`, so this step doesn’t affect the results).\n",
    "\n",
    "3. **Calculating Accuracy and F1 Score:**\n",
    "    - `accuracy_score` calculates the proportion of predictions that are correct based on the margin.\n",
    "    - `f1_score` calculates the F1 score, which is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "4. **Calculating Regression Metrics:**\n",
    "    - `mean_squared_error (MSE)` calculates the average squared difference between predicted and true values, indicating the overall error of the predictions.\n",
    "    - `r2_score (R²)` measures the proportion of variance in the true values that is explained by the model, with values closer to 1 indicating better fit.\n",
    "    - `spearmanr` calculates Spearman’s rank correlation coefficient, measuring the monotonic relationship between predicted and true values. A value close to 1 indicates a strong positive correlation.\n",
    "\n",
    "5. **Calculating NDCG (Normalized Discounted Cumulative Gain):**\n",
    "    - The true and predicted scores are grouped by the \"Query\" column to calculate the ranking scores for each query.\n",
    "    - The `ndcg_score` is calculated for each query by comparing the true and predicted ranked lists. It measures how well the model's ranking matches the true ranking.\n",
    "    - The average NDCG score across all queries is then computed using `np.mean()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0252\n",
      "R-squared (R²): -0.4765\n",
      "Spearman's Rank Correlation: 0.4983\n",
      "NDCG Mean Score: 0.9398\n",
      "- NDCG for 'bilirubin in plasma': 0.9326\n",
      "- NDCG for 'calcium in serum': 0.9494\n",
      "- NDCG for 'glucose in blood': 0.9381\n",
      "- NDCG for 'white blood cells count': 0.9391\n"
     ]
    }
   ],
   "source": [
    "y_true = df_test.loc[X_test.index, \"Actual Score\"]  \n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "\n",
    "y_true_grouped = df_test.groupby(\"Query\")[\"Actual Score\"].apply(list).tolist()\n",
    "y_pred_grouped = df_test.groupby(\"Query\")[\"Predicted Score\"].apply(list).tolist()\n",
    "\n",
    "ndcg_scores = [ndcg_score([true], [pred]) for true, pred in zip(y_true_grouped, y_pred_grouped)]\n",
    "ndcg_mean = np.mean(ndcg_scores) \n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}\")\n",
    "print(f\"NDCG Mean Score: {ndcg_mean:.4f}\")\n",
    "\n",
    "queries = df_test.groupby(\"Query\").first().index.tolist()\n",
    "\n",
    "for query, true, pred in zip(queries, y_true_grouped, y_pred_grouped):\n",
    "    ndcg = ndcg_score([true], [pred])\n",
    "    print(f\"- NDCG for '{query}': {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
