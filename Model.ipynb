{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation, reset_parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "1. **Reading the CSV File:**\n",
    "    - The code reads a CSV file into a Pandas DataFrame.\n",
    "\n",
    "2. **Shuffling the Data:**\n",
    "    - It shuffles the DataFrame's rows randomly using `sample()` and resets the index.\n",
    "\n",
    "3. **Encoding Categorical Columns:**\n",
    "    - A loop encodes several categorical columns (e.g., \"Query\", \"Name\", etc.) into integer values and creates new columns with the encoded values.\n",
    "\n",
    "4. **Creating a 'Score_label' Column:**\n",
    "    - It generates a new `Score_label` column by scaling and converting the `Normalized_Score` to integers.\n",
    "\n",
    "5. **Displaying the Data:**\n",
    "    - Finally, the first few rows of the updated DataFrame are printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Query LOINC Code  \\\n",
      "0         calcium in serum   104877-6   \n",
      "1  white blood cells count    14859-3   \n",
      "2         glucose in blood     2612-0   \n",
      "3      bilirubin in plasma    54352-0   \n",
      "4           cells in urine    12733-2   \n",
      "\n",
      "                                                Name  \\\n",
      "0  hematocrit volume fraction blood 1 5 hour post...   \n",
      "1  leukocyte leukocyte peritoneal fluid manual count   \n",
      "2             methemalbumin mass volume serum plasma   \n",
      "3                         nitrogen mass volume urine   \n",
      "4    tryptase enzymatic activity volume serum plasma   \n",
      "\n",
      "                           Component         System            Property  \\\n",
      "0  hematocrit 1 5h post dose glucose          blood                 vfr   \n",
      "1                leukocyte leukocyte  periton fluid                 nfr   \n",
      "2                      methemalbumin   serum plasma  mass concentration   \n",
      "3                           nitrogen          urine  mass concentration   \n",
      "4                           tryptase   serum plasma  cell concentration   \n",
      "\n",
      "                 Measurement  Normalized_Score  Query_encoded  Name_encoded  \\\n",
      "0            volume fraction          0.099502              1          3627   \n",
      "1                        NaN          0.114982              4          4018   \n",
      "2                mass volume          0.114856              3          4387   \n",
      "3                mass volume          0.105484              0          4636   \n",
      "4  enzymatic activity volume          0.113472              2          5414   \n",
      "\n",
      "   Component_encoded  System_encoded  Property_encoded  Measurement_encoded  \\\n",
      "0               1729              13                69                   45   \n",
      "1               1962              81                35                   -1   \n",
      "2               2084             113                24                   22   \n",
      "3               2173             140                24                   22   \n",
      "4               2601             113                 5                   12   \n",
      "\n",
      "   Score  \n",
      "0      0  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Datasets_with_scores/dataset_enhanced_5.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for col in [\"Query\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\"]:\n",
    "    df[f\"{col}_encoded\"] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score'] = (df['Normalized_Score'] * 10).astype(int).clip(0, 4) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "1. **Splitting the Data:**\n",
    "    - The `train_test_split()` function is used to split the DataFrame `df` into training and testing sets.\n",
    "    - 80% of the data is used for training (`train_data`), and 20% is reserved for testing (`test_data`).\n",
    "\n",
    "2. **Displaying Data Sizes:**\n",
    "    - The sizes of the training and testing sets are printed using `shape[0]`, which gives the number of rows in each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  21603\n",
      "Test data size:  7202\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Train data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for trainning\n",
    "\n",
    "1. **Defining Features:**\n",
    "    - A list of feature columns (`features`) is created, which includes the encoded versions of categorical columns (e.g., \"Query_encoded\", \"Name_encoded\", etc.).\n",
    "\n",
    "2. **Preparing Training Data:**\n",
    "    - `X_train` contains the feature values from `train_data`.\n",
    "    - `y_train` contains the target values (`Score_label`) from `train_data`.\n",
    "    - `q_train` calculates the size of each group based on the \"Query_encoded\" column using `groupby()`.\n",
    "\n",
    "3. **Preparing Testing Data:**\n",
    "    - `X_test` contains the feature values from `test_data`.\n",
    "    - `y_test` contains the target values (`Score_label`) from `test_data`.\n",
    "    - `q_test` calculates the size of each group based on the \"Query_encoded\" column for the test data.\n",
    "\n",
    "4. **Creating LightGBM Datasets:**\n",
    "    - `train_data` and `test_data` are converted into LightGBM datasets (`lgb.Dataset`), which are required for training a LightGBM model. These datasets include the feature data (`X_train`, `X_test`), target labels (`y_train`, `y_test`), and group sizes (`q_train`, `q_test`).\n",
    "\n",
    "5. **Displaying Training and Testing Data:**\n",
    "    - The first few rows of `X_train` and `X_test` are printed to verify the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "7417               1          4465               2111              32   \n",
      "11031              4           813                371             113   \n",
      "24328              4          1222                561             150   \n",
      "14999              2          3003               1505             113   \n",
      "5504               1          2926               1396             113   \n",
      "\n",
      "       Property_encoded  Measurement_encoded  \n",
      "7417                 35                   -1  \n",
      "11031                24                   22  \n",
      "24328                35                   -1  \n",
      "14999                24                   22  \n",
      "5504                 24                   22  \n",
      "       Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "6011               0          4466               2111              32   \n",
      "26323              2           493                214             140   \n",
      "18149              1          1644                774              13   \n",
      "2141               0          5574               2659              33   \n",
      "12462              1           832                340              65   \n",
      "\n",
      "       Property_encoded  Measurement_encoded  \n",
      "6011                 35                   -1  \n",
      "26323                61                   29  \n",
      "18149                35                   -1  \n",
      "2141                 35                   -1  \n",
      "12462                54                   27  \n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"Query_encoded\", \"Name_encoded\", \"Component_encoded\",\n",
    "    \"System_encoded\", \"Property_encoded\", \"Measurement_encoded\"\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Score\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Score\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train, free_raw_data=False)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data, free_raw_data=False)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "1. **Setting Hyperparameters:**\n",
    "    - The model uses `rank_xendcg` as the objective and `ndcg` as the evaluation metric\n",
    "    - A dictionary `params` is defined to specify the hyperparameters for training a LightGBM model. These include:\n",
    "        - `objective`: The type of learning task (\"rank_xendcg\" for ranking tasks).\n",
    "        - `metric`: Evaluation metric used (\"ndcg\" for normalized discounted cumulative gain).\n",
    "        - `boosting_type`: Boosting method (\"gbdt\" for Gradient Boosting Decision Trees).\n",
    "        - `num_leaves`: Number of leaves in the tree.\n",
    "        - `learning_rate`: Step size for each iteration.\n",
    "        - `max_depth`: Maximum depth of the tree.\n",
    "        - `verbosity`: Controls the amount of output during training.\n",
    "        - `lambda_l1` and `lambda_l2`: L1 and L2 regularization terms.\n",
    "        - `colsample_bytree`: Fraction of features to be used for each tree.\n",
    "        - `label_gain`: Prioritizes higher-ranking samples.\n",
    "        - `n_estimators`: sets a high number of rounds for better performance\n",
    "\n",
    "2. **Adaptive Training with AdaRank:**\n",
    "    \n",
    "    Since LightGBM doesn’t directly support *AdaRank*, this code simulates its boosting and reweighting mechanism using *LightGBM's* as the base model.  \n",
    "    - The model trains over multiple iterations (`n_iterations`), updating weights based on prediction errors.  \n",
    "    - Misclassified samples get higher weights, forcing the model to focus on harder cases.  \n",
    "    - **Early stopping** prevents overfitting, while **log evaluation** tracks performance every 100 rounds.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaRank Iteration 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 0.767888\tvalid_0's ndcg@3: 0.647153\tvalid_0's ndcg@4: 0.639226\tvalid_0's ndcg@5: 0.66032\n",
      "AdaRank Iteration 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 6\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 7\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 9\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 11\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 12\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 13\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 14\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 15\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 16\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 17\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 18\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 19\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "AdaRank Iteration 23\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 24\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 25\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 26\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 27\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 28\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 29\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 30\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 31\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 32\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 33\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 34\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 35\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 36\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 37\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 38\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 39\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 40\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 41\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 42\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 43\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 44\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 45\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 46\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 47\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 48\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 49\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n",
      "AdaRank Iteration 50\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 0.975508\n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    \"objective\": \"rank_xendcg\",  \n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,  \n",
    "    \"learning_rate\": 0.005,  \n",
    "    \"max_depth\": 7,  \n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.001,  \n",
    "    \"lambda_l2\": 0.001,\n",
    "    \"colsample_bytree\": 0.9,  \n",
    "    \"subsample\": 0.8,\n",
    "    \"min_child_samples\": 20,  \n",
    "    \"min_child_weight\": 0.01,\n",
    "    \"max_position\": 10,\n",
    "    \"feature_fraction\": 0.95,\n",
    "    \"label_gain\": [0, 1, 3, 7, 15],\n",
    "    \"n_estimators\": 10000,\n",
    "}\n",
    "\n",
    "n_iterations = 50\n",
    "weights = np.ones(len(X_train))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"AdaRank Iteration {i + 1}\")\n",
    "\n",
    "    model = lgb.train(\n",
    "        base_params,\n",
    "        train_data,\n",
    "        valid_sets=[test_data],\n",
    "        num_boost_round=500,\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    errors = np.abs(y_train - y_pred)\n",
    "    weights *= 1 + (errors / (errors.std() + 1e-6)) * 0.001\n",
    "    weights = np.log1p(weights)\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, group=q_train, weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "1. **Making Predictions:**\n",
    "    - `y_pred = model.predict(X_test)` uses the trained LightGBM model to make predictions on the test data (`X_test`).\n",
    "\n",
    "2. **Scaling the Predictions:**\n",
    "    - A `MinMaxScaler` is initialized to scale the predictions (`y_pred`) to a range between 0 and 1.\n",
    "    - The predictions are reshaped, scaled, and then flattened back to the original shape.\n",
    "\n",
    "3. **Creating a DataFrame for the Test Data:**\n",
    "    - `df_test` is created by copying `X_test`.\n",
    "    - New columns are added to `df_test`:\n",
    "        - `Predicted Score`: The scaled predictions.\n",
    "        - `Actual Score`: The actual values from the `Normalized_Score` column in the original DataFrame (`df`).\n",
    "        - `Name` and `Query`: The corresponding values from the original DataFrame (`df`).\n",
    "\n",
    "4. **Sorting the Results:**\n",
    "    - The DataFrame is sorted by `Query` and `Predicted Score` in ascending order by `Query` and descending order by `Predicted Score`.\n",
    "\n",
    "5. **Saving the Results:**\n",
    "    - The sorted DataFrame `df_test` is saved to a CSV file called \"results.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to ./Results/results.csv\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"./Results/results.csv\"\n",
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted Score\"] = y_pred\n",
    "df_test[\"Actual Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "df_test[\"LOINC Code\"] = df.loc[X_test.index, \"LOINC Code\"]\n",
    "\n",
    "df_test = df_test[[\"LOINC Code\", \"Query\", \"Name\", \"Predicted Score\", \"Actual Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(output_filename, mode='a', index=False, header=not os.path.exists(output_filename))\n",
    "\n",
    "print(f\"Ranked results saved to {output_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
