{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation, reset_parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import ndcg_score\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "1. **Reading the CSV File:**\n",
    "    - The code reads a CSV file into a Pandas DataFrame.\n",
    "\n",
    "2. **Shuffling the Data:**\n",
    "    - It shuffles the DataFrame's rows randomly using `sample()` and resets the index.\n",
    "\n",
    "3. **Encoding Categorical Columns:**\n",
    "    - A loop encodes several categorical columns (e.g., \"Query\", \"Name\", etc.) into integer values and creates new columns with the encoded values.\n",
    "\n",
    "4. **Creating a 'Score_label' Column:**\n",
    "    - It generates a new `Score_label` column by scaling and converting the `Normalized_Score` to integers.\n",
    "\n",
    "5. **Displaying the Data:**\n",
    "    - Finally, the first few rows of the updated DataFrame are printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Query LOINC Code  \\\n",
      "0         glucose in blood    62245-6   \n",
      "1           cells in urine    53227-5   \n",
      "2         glucose in blood      807-8   \n",
      "3  white blood cells count    26471-3   \n",
      "4           cells in urine    99865-8   \n",
      "\n",
      "                                                Name  \\\n",
      "0  nucleated erythrocyte leukocyte ratio blood fe...   \n",
      "1              leukocyte area cervix wet preparation   \n",
      "2     leukocyte volume pleural fluid automated count   \n",
      "3                          leukocyte leukocyte blood   \n",
      "4  acanthocyte presence urine sediment computer a...   \n",
      "\n",
      "                         Component       System              Property  \\\n",
      "0  erythrocyte nucleated leukocyte  blood fetus                 ratio   \n",
      "1                        leukocyte          cvx                 naric   \n",
      "2                        leukocyte    plr field  number concentration   \n",
      "3              leukocyte leukocyte        blood                   nfr   \n",
      "4                      acanthocyte    urine sed                 prthr   \n",
      "\n",
      "  Measurement  Status    Units  Normalized_Score  Query_encoded  Name_encoded  \\\n",
      "0       ratio  active  100 wbc          0.173140              3          1008   \n",
      "1        area  active      hpf          0.098562              2           577   \n",
      "2      volume  active  10 3 ul          0.064983              3           718   \n",
      "3         NaN  active      NaN          0.262769              4           613   \n",
      "4    presence  active      NaN          0.174694              2            15   \n",
      "\n",
      "   Component_encoded  System_encoded  Property_encoded  Measurement_encoded  \\\n",
      "0                142              12                25                   19   \n",
      "1                182              20                16                    0   \n",
      "2                182              29                22                   22   \n",
      "3                199               8                17                   -1   \n",
      "4                  0              45                24                   17   \n",
      "\n",
      "   Score_label  \n",
      "0            1  \n",
      "1            0  \n",
      "2            0  \n",
      "3            2  \n",
      "4            1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_scores_basic.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for col in [\"Query\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\"]:\n",
    "    df[f\"{col}_encoded\"] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score_label'] = (df['Normalized_Score'] * 10).astype(int).clip(0, 4) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "1. **Splitting the Data:**\n",
    "    - The `train_test_split()` function is used to split the DataFrame `df` into training and testing sets.\n",
    "    - 80% of the data is used for training (`train_data`), and 20% is reserved for testing (`test_data`).\n",
    "\n",
    "2. **Displaying Data Sizes:**\n",
    "    - The sizes of the training and testing sets are printed using `shape[0]`, which gives the number of rows in each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  5779\n",
      "Test data size:  1445\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train data size: \", train_data.shape[0])\n",
    "print(\"Test data size: \", test_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for trainning\n",
    "\n",
    "1. **Defining Features:**\n",
    "    - A list of feature columns (`features`) is created, which includes the encoded versions of categorical columns (e.g., \"Query_encoded\", \"Name_encoded\", etc.).\n",
    "\n",
    "2. **Preparing Training Data:**\n",
    "    - `X_train` contains the feature values from `train_data`.\n",
    "    - `y_train` contains the target values (`Score_label`) from `train_data`.\n",
    "    - `q_train` calculates the size of each group based on the \"Query_encoded\" column using `groupby()`.\n",
    "\n",
    "3. **Preparing Testing Data:**\n",
    "    - `X_test` contains the feature values from `test_data`.\n",
    "    - `y_test` contains the target values (`Score_label`) from `test_data`.\n",
    "    - `q_test` calculates the size of each group based on the \"Query_encoded\" column for the test data.\n",
    "\n",
    "4. **Creating LightGBM Datasets:**\n",
    "    - `train_data` and `test_data` are converted into LightGBM datasets (`lgb.Dataset`), which are required for training a LightGBM model. These datasets include the feature data (`X_train`, `X_test`), target labels (`y_train`, `y_test`), and group sizes (`q_train`, `q_test`).\n",
    "\n",
    "5. **Displaying Training and Testing Data:**\n",
    "    - The first few rows of `X_train` and `X_test` are printed to verify the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "6110              1          1046                273               8   \n",
      "4962              2           580                182              34   \n",
      "4152              3           628                199              29   \n",
      "911               2           646                203              16   \n",
      "6541              2           928                242              29   \n",
      "\n",
      "      Property_encoded  Measurement_encoded  \n",
      "6110                17                   -1  \n",
      "4962                16                    0  \n",
      "4152                17                   -1  \n",
      "911                 21                   -1  \n",
      "6541                17                   -1  \n",
      "      Query_encoded  Name_encoded  Component_encoded  System_encoded  \\\n",
      "1593              1           414                138              45   \n",
      "6952              2            61                  8              14   \n",
      "1790              3            22                253              14   \n",
      "4348              3           234                 98              36   \n",
      "6165              4           210                 58              36   \n",
      "\n",
      "      Property_encoded  Measurement_encoded  \n",
      "1593                22                   22  \n",
      "6952                18                   18  \n",
      "1790                17                   -1  \n",
      "4348                12                   13  \n",
      "6165                28                   14  \n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"Query_encoded\", \"Name_encoded\", \"Component_encoded\",\n",
    "    \"System_encoded\", \"Property_encoded\", \"Measurement_encoded\"\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Score_label\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[\"Score_label\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "1. **Setting Hyperparameters:**\n",
    "    - A dictionary `params` is defined to specify the hyperparameters for training a LightGBM model. These include:\n",
    "        - `objective`: The type of learning task (\"rank_xendcg\" for ranking tasks).\n",
    "        - `metric`: Evaluation metric used (\"ndcg\" for normalized discounted cumulative gain).\n",
    "        - `boosting_type`: Boosting method (\"gbdt\" for Gradient Boosting Decision Trees).\n",
    "        - `num_leaves`: Number of leaves in the tree.\n",
    "        - `learning_rate`: Step size for each iteration.\n",
    "        - `max_depth`: Maximum depth of the tree.\n",
    "        - `verbosity`: Controls the amount of output during training.\n",
    "        - `lambda_l1` and `lambda_l2`: L1 and L2 regularization terms.\n",
    "        - `colsample_bytree`: Fraction of features to be used for each tree.\n",
    "\n",
    "2. **Training the LightGBM Model:**\n",
    "    - The `lgb.train()` function is used to train the LightGBM model with the specified `params`, using `train_data` as the training dataset.\n",
    "    - `valid_sets=[test_data]` specifies the test dataset to be used for validation during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[200]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[300]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[400]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[500]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 0.855575\tvalid_0's ndcg@3: 0.889463\tvalid_0's ndcg@4: 0.843038\tvalid_0's ndcg@5: 0.863632\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"rank_xendcg\",  \n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 20,  \n",
    "    \"learning_rate\": 0.05,  \n",
    "    \"max_depth\": 6,  \n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.001,  \n",
    "    \"lambda_l2\": 0.001,\n",
    "    \"colsample_bytree\": 0.9,  \n",
    "    \"subsample\": 0.8,\n",
    "    \"min_child_samples\": 20,  \n",
    "    \"min_child_weight\": 0.01,\n",
    "    \"max_position\": 10,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"label_gain\": [0, 1, 3, 7, 15],  \n",
    "    \"n_estimators\": 10000,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[test_data],\n",
    "    num_boost_round=5000,\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=500), \n",
    "        log_evaluation(100),\n",
    "        reset_parameter(learning_rate=lambda iter: 0.05 * (0.99 ** iter))\n",
    "]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "1. **Making Predictions:**\n",
    "    - `y_pred = model.predict(X_test)` uses the trained LightGBM model to make predictions on the test data (`X_test`).\n",
    "\n",
    "2. **Scaling the Predictions:**\n",
    "    - A `MinMaxScaler` is initialized to scale the predictions (`y_pred`) to a range between 0 and 1.\n",
    "    - The predictions are reshaped, scaled, and then flattened back to the original shape.\n",
    "\n",
    "3. **Creating a DataFrame for the Test Data:**\n",
    "    - `df_test` is created by copying `X_test`.\n",
    "    - New columns are added to `df_test`:\n",
    "        - `Predicted_Score`: The scaled predictions.\n",
    "        - `Actual_Score`: The actual values from the `Normalized_Score` column in the original DataFrame (`df`).\n",
    "        - `Name` and `Query`: The corresponding values from the original DataFrame (`df`).\n",
    "\n",
    "4. **Sorting the Results:**\n",
    "    - The DataFrame is sorted by `Query` and `Predicted_Score` in ascending order by `Query` and descending order by `Predicted_Score`.\n",
    "\n",
    "5. **Saving the Results:**\n",
    "    - The sorted DataFrame `df_test` is saved to a CSV file called \"ranked_results.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to {output_filename}\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"results.csv\"\n",
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted Score\"] = y_pred\n",
    "df_test[\"Actual Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "df_test[\"LOINC Code\"] = df.loc[X_test.index, \"LOINC Code\"]\n",
    "\n",
    "df_test = df_test[[\"LOINC Code\", \"Query\", \"Name\", \"Predicted Score\", \"Actual Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(output_filename, mode='a', index=False, header=not os.path.exists(output_filename))\n",
    "\n",
    "print(\"Ranked results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "1. **Extracting True Scores:**\n",
    "    - Extract the true scores (`Actual_Score`) from the `df_test` DataFrame corresponding to the indices of `X_test`.\n",
    "\n",
    "2. **Adjusting Predictions and True Scores:**\n",
    "    - A margin of 0.05 is defined to determine how close the predicted values should be to the actual values.\n",
    "    - `y_pred_adjusted` checks if the absolute difference between predicted and actual scores is within the margin, essentially marking whether the prediction is considered \"correct.\"\n",
    "    - `y_true_adjusted` checks if the true scores are within the margin of themselves (which will always be `True`, so this step doesn’t affect the results).\n",
    "\n",
    "3. **Calculating Accuracy and F1 Score:**\n",
    "    - `accuracy_score` calculates the proportion of predictions that are correct based on the margin.\n",
    "    - `f1_score` calculates the F1 score, which is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "4. **Calculating Regression Metrics:**\n",
    "    - `mean_squared_error (MSE)` calculates the average squared difference between predicted and true values, indicating the overall error of the predictions.\n",
    "    - `r2_score (R²)` measures the proportion of variance in the true values that is explained by the model, with values closer to 1 indicating better fit.\n",
    "    - `spearmanr` calculates Spearman’s rank correlation coefficient, measuring the monotonic relationship between predicted and true values. A value close to 1 indicates a strong positive correlation.\n",
    "\n",
    "5. **Calculating NDCG (Normalized Discounted Cumulative Gain):**\n",
    "    - The true and predicted scores are grouped by the \"Query\" column to calculate the ranking scores for each query.\n",
    "    - The `ndcg_score` is calculated for each query by comparing the true and predicted ranked lists. It measures how well the model's ranking matches the true ranking.\n",
    "    - The average NDCG score across all queries is then computed using `np.mean()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0285\n",
      "R-squared (R²): -0.0209\n",
      "Spearman's Rank Correlation: 0.6285\n",
      "NDCG Mean Score: 0.8845\n",
      "- NDCG for 'bilirubin in plasma': 0.8615\n",
      "- NDCG for 'calcium in serum': 0.9149\n",
      "- NDCG for 'cells in urine': 0.8793\n",
      "- NDCG for 'glucose in blood': 0.9091\n",
      "- NDCG for 'white blood cells count': 0.8577\n"
     ]
    }
   ],
   "source": [
    "y_true = df_test.loc[X_test.index, \"Actual Score\"]  \n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "\n",
    "y_true_grouped = df_test.groupby(\"Query\")[\"Actual Score\"].apply(list).tolist()\n",
    "y_pred_grouped = df_test.groupby(\"Query\")[\"Predicted Score\"].apply(list).tolist()\n",
    "\n",
    "ndcg_scores = [ndcg_score([true], [pred]) for true, pred in zip(y_true_grouped, y_pred_grouped)]\n",
    "ndcg_mean = np.mean(ndcg_scores) \n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}\")\n",
    "print(f\"NDCG Mean Score: {ndcg_mean:.4f}\")\n",
    "\n",
    "queries = df_test.groupby(\"Query\").first().index.tolist()\n",
    "\n",
    "for query, true, pred in zip(queries, y_true_grouped, y_pred_grouped):\n",
    "    ndcg = ndcg_score([true], [pred])\n",
    "    print(f\"- NDCG for '{query}': {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
