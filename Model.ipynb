{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, r2_score\n",
    "from scipy.stats import spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Query LOINC Code  \\\n",
      "0      bilirubin in plasma     1971-1   \n",
      "1         glucose in blood     1920-8   \n",
      "2         glucose in blood     1751-7   \n",
      "3  white blood cells count     2069-3   \n",
      "4      bilirubin in plasma    54439-5   \n",
      "\n",
      "                                      Name                     Component  \\\n",
      "0          bilirubin indirect serum plasma  bilirubin non glucuronidated   \n",
      "1  aspartate aminotransferase serum plasma    aspartate aminotransferase   \n",
      "2                     albumin serum plasma                       albumin   \n",
      "3                           chloride blood                      chloride   \n",
      "4         calcium bilirubinate total stone    calcium bilirubinate total   \n",
      "\n",
      "         System                 Property                Measurement  \\\n",
      "0  serum plasma       mass concentration                mass volume   \n",
      "1  serum plasma       cell concentration  enzymatic activity volume   \n",
      "2  serum plasma       mass concentration                mass volume   \n",
      "3         blood  substance concentration                mole volume   \n",
      "4      calculus                      mfr                        NaN   \n",
      "\n",
      "   Normalized_Score  Query_encoded  Name_encoded  Score_label  \n",
      "0          0.599738              0            14            5  \n",
      "1          0.221357              1            11            2  \n",
      "2          0.155101              1             5            1  \n",
      "3          0.384412              2            32            3  \n",
      "4          0.350481              0            23            3  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_with_scores.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df[\"Query_encoded\"] = df[\"Query\"].astype(\"category\").cat.codes\n",
    "df[\"Name_encoded\"] = df[\"Name\"].astype(\"category\").cat.codes\n",
    "\n",
    "df['Score_label'] = (df['Normalized_Score'] * 10).astype(int) \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Query_encoded  Name_encoded\n",
      "106              1            61\n",
      "14               2            52\n",
      "92               0            30\n",
      "179              2            61\n",
      "102              2            51\n",
      "     Query_encoded  Name_encoded\n",
      "135              2            65\n",
      "137              0            44\n",
      "164              1            31\n",
      "76               1            16\n",
      "79               0             5\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = train_data[[\n",
    "    \"Query_encoded\",   \n",
    "    \"Name_encoded\"\n",
    "]]\n",
    "y_train = train_data[\"Score_label\"]\n",
    "q_train = train_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "X_test = test_data[[\n",
    "    \"Query_encoded\",   \n",
    "    \"Name_encoded\"\n",
    "]]\n",
    "y_test = test_data[\"Score_label\"]\n",
    "q_test = test_data.groupby(\"Query_encoded\").size().values  \n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=q_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=q_test, reference=train_data)\n",
    "\n",
    "print(X_train.tail())\n",
    "print(X_test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",  \n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 20,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 20,\n",
    "    \"verbosity\": -1,\n",
    "    \"lambda_l1\": 0.05,\n",
    "    \"lambda_l2\": 0.05,\n",
    "    \"colsample_bytree\": 0.9\n",
    "}\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked results saved to 'ranked_results.csv'\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)  \n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_pred = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "\n",
    "df_test[\"Predicted_Score\"] = y_pred\n",
    "df_test[\"Actual_Score\"] = df.loc[X_test.index, \"Normalized_Score\"]\n",
    "df_test[\"Name\"] = df.loc[X_test.index, \"Name\"]\n",
    "df_test[\"Query\"] = df.loc[X_test.index, \"Query\"]\n",
    "\n",
    "df_test = df_test[[\"Query\", \"Name\", \"Predicted_Score\", \"Actual_Score\"]]\n",
    "\n",
    "df_test = df_test.sort_values(by=[\"Query\", \"Predicted_Score\"], ascending=[True, False])\n",
    "\n",
    "df_test.to_csv(\"ranked_results.csv\", index=False)\n",
    "\n",
    "print(\"Ranked results saved to 'ranked_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5122\n",
      "F1 Score: 0.6774\n",
      "Mean Squared Error (MSE): 0.0385\n",
      "R-squared (R²): 0.1863\n",
      "Spearman's Rank Correlation: 0.4294\n"
     ]
    }
   ],
   "source": [
    "y_true = df_test.loc[X_test.index, \"Actual_Score\"]  \n",
    "\n",
    "margin = 0.1\n",
    "\n",
    "y_pred_adjusted = np.abs(y_pred - y_true) <= margin  \n",
    "y_true_adjusted = np.abs(y_true - y_true) <= margin  \n",
    "\n",
    "accuracy = accuracy_score(y_true_adjusted, y_pred_adjusted)\n",
    "f1 = f1_score(y_true_adjusted, y_pred_adjusted)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
