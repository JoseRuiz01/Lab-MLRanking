{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Mapping Dictionary\n",
    "The `query_mapping` dictionary links medical queries to their respective components and systems:\n",
    "\n",
    "- **\"glucose in blood\"** → Component: `glucose`, System: `blood`\n",
    "- **\"bilirubin in plasma\"** → Component: `bilirubin`, System: `plasma`\n",
    "- **\"white blood cells count\"** → Component: `leukocytes`, System: `blood`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mapping = {\n",
    "    \"glucose in blood\": {\n",
    "        \"component\": \"glucose\",\n",
    "        \"system\": \"blood\"\n",
    "    },\n",
    "    \"bilirubin in plasma\": {\n",
    "        \"component\": \"bilirubin\",\n",
    "        \"system\": \"plasma\"\n",
    "    },\n",
    "    \"white blood cells count\": {\n",
    "        \"component\": \"leukocytes\",\n",
    "        \"system\": \"blood\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Files\n",
    "The code loads the Excel file `loinc_dataset-v2.xlsx` using **Pandas**:\n",
    "\n",
    "### 1. **Iterating Through Sheets**\n",
    "It iterates through each sheet in the Excel file:\n",
    "- Parses the data, starting from the third row (`header=2`).\n",
    "- Converts the sheet name to lowercase.\n",
    "- Prints the first 5 rows of the sheet to inspect the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./loinc_dataset-v2.xlsx\"\n",
    "xl = pd.ExcelFile(file_path)\n",
    "\n",
    "for sheet_name in xl.sheet_names:\n",
    "    query_df = xl.parse(sheet_name, header=2)\n",
    "    sheet_name = sheet_name.lower() \n",
    "    \n",
    "    print(f\"First 5 rows of sheet: {sheet_name}\")\n",
    "    print(query_df.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Renaming and Extracting Measurement Type\n",
    "\n",
    "This script performs two data cleaning operations:\n",
    "\n",
    "1. **Renaming Columns**  \n",
    "2. **Extracting and Removing Measurement Types from Names**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet_name in xl.sheet_names:\n",
    "    query_df = xl.parse(sheet_name, header=2)\n",
    "    sheet_name = sheet_name.lower() \n",
    "    query_df.rename(columns={\"long_common_name\": \"name\"}, inplace=True)\n",
    "\n",
    "    query_df[\"measurement_type\"] = query_df[\"name\"].apply(lambda x: re.findall(r\"\\[(.*?)\\]\", x)[0] if \"[\" in x else \"\")\n",
    "    query_df[\"name\"] = query_df[\"name\"].apply(lambda x: re.sub(r\"\\[.*?\\]\", \"\", x).strip() if isinstance(x, str) else x)\n",
    "\n",
    "    print(f\"First 5 rows of sheet: {sheet_name}\")\n",
    "    print(query_df.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbreviation Mapping, Stop Words, and Lemmatization\n",
    "\n",
    "This script performs **text preprocessing** by:\n",
    "- Expanding **abbreviations** into full terms using the dictionary `abbreviation_mapping`.\n",
    "- Removing **common stop words**.\n",
    "- Applying **lemmatization** to reduce words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_mapping = {\n",
    "    'c': 'component',\n",
    "    'mcnc': 'mass concentration',\n",
    "    'bld': 'blood',\n",
    "    'scnc': 'substance concentration',\n",
    "    'susc': 'susceptibility',\n",
    "    'acnc': 'amount concentration',\n",
    "    'plas': 'plasma',\n",
    "    'ccnc': 'cell concentration',\n",
    "    'ncnc': 'number concentration',\n",
    "    'XXX': 'unknown',\n",
    "    'bpu': 'blood product unit',\n",
    "    'fld': 'fluid',\n",
    "    'abo': 'abo blood group',\n",
    "    'ser': 'serum',\n",
    "    'mscnc': 'mass substance concentration',\n",
    "    'bbl': 'blood product unit',\n",
    "    'rbc': 'red blood cells', \n",
    "    'blda': 'blood group a',\n",
    "    'bldv': 'blood group v', \n",
    "    'tiss': 'tissue',\n",
    "    'bldco': 'coord blood',\n",
    "    'csf': 'cerebrospinal fluid'\n",
    "}\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing in a DataFrame\n",
    "\n",
    "This script performs text cleaning and abbreviation replacement on text columns in a DataFrame (`df`). It helps standardize text data for further analysis by removing noise and ensuring consistency.\n",
    "\n",
    "## Functions\n",
    "\n",
    "### 1. clean_text\n",
    "Cleans a given text string by:\n",
    "1. Converting it to **lowercase**.\n",
    "2. Removing **punctuation** (replacing non-alphanumeric characters with spaces).\n",
    "3. **Tokenizing** (splitting the text into words).\n",
    "4. Removing **stop words** (common words that don't contribute much meaning).\n",
    "5. Applying **lemmatization** (reducing words to their base form).\n",
    "\n",
    "If the input is not a string, it returns an empty string.\n",
    "\n",
    "### 2. replace_abbreviations\n",
    "Replaces known abbreviations in a given text using the `abbreviation_mapping` dictionary.\n",
    "- Splits the text into words.\n",
    "- Replaces each word if it exists in the abbreviation dictionary.\n",
    "- Returns the modified text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower() \n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)  \n",
    "        words = text.split()  \n",
    "        words = [word for word in words if word not in stop_words]  \n",
    "        words = [lemmatizer.lemmatize(word) for word in words] \n",
    "        return \" \".join(words)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def replace_abbreviations(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        words = [abbreviation_mapping.get(word, word) for word in words]  \n",
    "        return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "for col in query_df.select_dtypes(include=[\"object\"]).columns:\n",
    "    if col != \"loinc_num\":  \n",
    "        query_df[col] = query_df[col].apply(clean_text)\n",
    "        query_df[col] = query_df[col].apply(replace_abbreviations)\n",
    "\n",
    "print(query_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Weights and Embedding Model Initialization\n",
    "\n",
    "This script defines **column weights** for a scoring system and initializes an **embedding model** for text similarity calculations.\n",
    "\n",
    "## Column Weights\n",
    "\n",
    "The `column_weights` dictionary assigns importance to different columns when calculating scores:\n",
    "\n",
    "- **Higher weights** (e.g., `name`, `component`) indicate greater importance in the scoring process.\n",
    "- `loinc_num` has a weight of **0** because it is likely an identifier and does not contribute to similarity calculations.\n",
    "\n",
    "## Embedding Model Initialization\n",
    "\n",
    "The script attempts to load an embedding model for text similarity using **SentenceTransformer**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_weights = {\n",
    "    'name': 1.5,\n",
    "    'component': 6.0,\n",
    "    'long_common_name': 1.0,\n",
    "    'system': 3.0,\n",
    "    'property': 1.0,\n",
    "    'measurement_type': 1.0,\n",
    "    'loinc_num': 0\n",
    "}\n",
    "\n",
    "global embedding_model\n",
    "if 'embedding_model' not in globals():\n",
    "    try:\n",
    "        embedding_model = SentenceTransformer('pritamdeka/BioBERT-MNLI')\n",
    "    except:\n",
    "        try:\n",
    "            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        except:\n",
    "            embedding_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Score Calculation\n",
    "This script calculates a **relevance score** for each row in a dataset by comparing a query to the dataset's text fields using both **traditional keyword matching** and **semantic similarity via embeddings**.\n",
    "\n",
    "\n",
    "### 1. calculate_score\n",
    "This function calculates the relevance score for a given row by:\n",
    "1. Splitting the query into words and storing them in a set.\n",
    "2. Initializing an empty dictionary to track matched words.\n",
    "3. Computing the **traditional** relevance score.\n",
    "4. Computing the **embedding-based** relevance score.\n",
    "5. Combining both scores and optionally printing debug information.\n",
    "\n",
    "\n",
    "### 2. get_query_embedding\n",
    "Encodes the query into an **embedding vector** using a pre-trained embedding model.\n",
    "- If the embedding model is available, it encodes the query.\n",
    "- If an error occurs, it prints an error message and returns `None`.\n",
    "\n",
    "### 3. calculate_traditional_score\n",
    "This function calculates a **keyword matching score** based on:\n",
    "- The presence of query words in the main columns `component` and `system`\n",
    "- A predefined weight assigned to each column.\n",
    "\n",
    "### 4. calculate_embedding_score\n",
    "This function computes the **semantic similarity score** between the query embedding and the row's text fields:\n",
    "- Encodes the text field into an embedding.\n",
    "- Uses **cosine similarity** to measure similarity between the query and field.\n",
    "- Converts the similarity score (ranging from -1 to 1) into a normalized range.\n",
    "- Applies column weights to adjust the score.\n",
    "- Adds the result to the final score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(query, query_df, row, debug=False):\n",
    "    query_embedding = get_query_embedding(query)\n",
    "    debug_info = {\"query\": query, \"embedding_score\": [], \"traditional_score\": []}\n",
    "    \n",
    "    traditional_score = calculate_traditional_score(query, row, debug_info)\n",
    "    embedding_score = calculate_embedding_score(query_embedding, query_df, row, debug_info)\n",
    "    score = traditional_score + embedding_score\n",
    "    \n",
    "    debug_info[\"final_score\"] = score\n",
    "    \n",
    "    if debug:\n",
    "        print(debug_info)\n",
    "   \n",
    "    return score\n",
    "\n",
    "\n",
    "def get_query_embedding(query):\n",
    "    if embedding_model:\n",
    "        try:\n",
    "            return embedding_model.encode(query.lower())\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding encoding error: {e}\")\n",
    "    return None\n",
    "\n",
    "def calculate_traditional_score(query, row, debug_info):\n",
    "    score = 0\n",
    "    \n",
    "    query_component = query_mapping[query][\"component\"].lower()\n",
    "    query_system = query_mapping[query][\"system\"].lower()\n",
    "\n",
    "    component = row.get(\"component\", \"\").lower() \n",
    "    system = row.get(\"system\", \"\").lower()  \n",
    "    \n",
    "    if query_component == component:\n",
    "        score += column_weights.get(\"component\", 1.0)  * column_weights.get(\"component\", 1.0) \n",
    "    elif query_component in component:\n",
    "        score += (column_weights.get(\"component\", 1.0) * 0.5) * column_weights.get(\"component\", 1.0)   \n",
    "    \n",
    "    if query_system == system:\n",
    "        score += column_weights.get(\"system\", 1.0)  * column_weights.get(\"system\", 1.0)  \n",
    "    elif query_system in system:\n",
    "        score += (column_weights.get(\"system\", 1.0) * 0.5) * column_weights.get(\"system\", 1.0)   \n",
    "\n",
    "    debug_info[\"traditional_score\"].append({\"score\": score})\n",
    "        \n",
    "    return score\n",
    "\n",
    "def calculate_embedding_score(query_embedding, query_df, row, debug_info):\n",
    "    score = 0\n",
    "    if embedding_model and query_embedding is not None:\n",
    "        for col in query_df.select_dtypes(include=[\"object\"]).columns:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                cell_text = str(row[col]).lower()\n",
    "                weight = column_weights.get(col, 1.0)\n",
    "                try:\n",
    "                    cell_embedding = embedding_model.encode(cell_text)\n",
    "                    similarity = cosine_similarity([query_embedding], [cell_embedding])[0][0]\n",
    "                    embedding_score = ((similarity + 1) / 2) * 5 * weight\n",
    "                    score += embedding_score\n",
    "                except Exception as e:\n",
    "                    print(f\"Embedding similarity error: {e}\")\n",
    "        debug_info[\"embedding_score\"].append({\"score\": score})\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the files\n",
    "This script processes an Excel file, calculates relevance scores for each row based on a query, normalizes the scores, and saves the results to a CSV file. The relevance scores are computed using both traditional keyword matching and embeddings.\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "   - Loads the Excel file and iterates over each sheet.\n",
    "   - Renames columns for consistency and extracts relevant information (e.g., measurement type) from the `name` column.\n",
    "   - Cleans and strips spaces from column names.\n",
    "\n",
    "### 2. Score Calculation\n",
    "   - For each row, a relevance score is computed using `calculate_score`.\n",
    "   - Debug information is printed for the first 5 rows.\n",
    "\n",
    "### 3.Normalization\n",
    "   - The minimum and maximum scores are calculated, and a new column for normalized scores is added using the formula `(score - min_score) / (max_score - min_score)`.\n",
    "\n",
    "### 4. Results\n",
    "   - The results (Query, LOINC Code, Name, Score, Normalized Score) are saved to a CSV file: `dataset_with_scores.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = float(\"inf\")\n",
    "max_score = float(\"-inf\")\n",
    "\n",
    "\n",
    "def preprocess(excel_file):\n",
    "    results = []\n",
    "    output_filename = \"./Datasets_with_scores/dataset_scores_basic.csv\"\n",
    "    xl = pd.ExcelFile(excel_file)\n",
    "\n",
    "    for sheet_name in xl.sheet_names:\n",
    "        query_df = xl.parse(sheet_name, header=2)\n",
    "        sheet_name = sheet_name.lower()\n",
    "         \n",
    "        query_df.rename(columns={\"long_common_name\": \"name\"}, inplace=True)\n",
    "        query_df[\"measurement_type\"] = query_df[\"name\"].apply(lambda x: re.findall(r\"\\[(.*?)\\]\", x)[0] if \"[\" in x else \"\")\n",
    "        query_df[\"name\"] = query_df[\"name\"].apply(lambda x: re.sub(r\"\\[.*?\\]\", \"\", x).strip() if isinstance(x, str) else x)\n",
    "\n",
    "        for col in query_df.select_dtypes(include=[\"object\"]).columns:\n",
    "            if col != \"loinc_num\":  \n",
    "                query_df[col] = query_df[col].apply(clean_text)\n",
    "                query_df[col] = query_df[col].apply(replace_abbreviations)\n",
    "            \n",
    "        query_df.columns = query_df.columns.str.strip()  \n",
    "        \n",
    "        if len(query_df.columns) < 4:\n",
    "            print(f\"Skipping sheet '{sheet_name}': Missing required columns\")\n",
    "            continue\n",
    "        \n",
    "        for _, row in query_df.iterrows():\n",
    "            score = calculate_score(sheet_name, query_df, row, debug=True if row.name < 5 else False)\n",
    "            results.append([sheet_name, row.iloc[0], row.iloc[1], row.iloc[2], row.iloc[3], row.iloc[4], row.iloc[5], score])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=[\"Query\", \"LOINC Code\", \"Name\", \"Component\", \"System\", \"Property\", \"Measurement\", \"Score\"])\n",
    "\n",
    "    min_score = results_df[\"Score\"].min()\n",
    "    max_score = results_df[\"Score\"].max()\n",
    "\n",
    "    results_df[\"Normalized_Score\"] = results_df[\"Score\"].apply(lambda score: (score - min_score) / (max_score - min_score) if max_score != min_score else 1.0)\n",
    "    results_df.drop(columns=[\"Score\"], inplace=True)\n",
    "\n",
    "    results_df.to_csv(output_filename, mode='a', index=False, header=not os.path.exists(output_filename))\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "\n",
    "results_df = preprocess(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
